{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PrvPXRrK1yV"
   },
   "source": [
    "# NLP 702 Deep Learning for Natural Language Processing\n",
    "## Finetuning T5 model \n",
    "\n",
    "### Goal of this tutorial:\n",
    "- Know the background of Text-to-Text Transfer Transformer (T5) model.\n",
    "- Learn how to finetune T5 model for sentiment analysis\n",
    "\n",
    "### References\n",
    "Some useful references:\n",
    "1. T5 Original Paper https://arxiv.org/pdf/1910.10683.pdf\n",
    "2. T5 HuggingFace blog https://huggingface.co/transformers/model_doc/t5.html\n",
    "3. T5 model card https://huggingface.co/t5-base \n",
    "4. T5 blog from Google AI https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html (material for T5 background is borrowed from this blog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1k__iZ8N_44"
   },
   "source": [
    "### T5 model - Background\n",
    "\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s1600/image2.png\" height=\"250\" width=\"550\"/>\n",
    "\n",
    "\n",
    "Text-to-Text Transfer Transformer (T5) model is an encoder-decoder model pretrained to fill in dropped-out spans of text (denoted by \\<M\\>) from documents in a large-scale unlabeled dataset. With T5, all NLP tasks can be reframed into a unified text-to-text format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. T5's text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). One can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s1600/image3.gif\" height=\"250\" width=\"550\"/>\n",
    "\n",
    "In the above illustration, T5 is flexibly finetuned on several (diverse) supervised tasks:\n",
    "- **Machine Translation** - Translate sentence from English to German \n",
    "- **Sentence Acceptability (CoLA)** - Classify if a given sentence is grammatically and syntactically acceptable\n",
    "- **Semantic Textual Similarity (STS)** - Predict how similar two given sentences are (regression task)\n",
    "- **Summarization** - Summarize a given passage\n",
    "\n",
    "### T5 model - Finetuning on sentiment analysis task\n",
    "\n",
    "In this tutorial we will focus on finetuning T5 model on sentiment analysis task. Specifically, we focus on classifying the sentiment of the tweet. We make use of the dataset provided by ``SemEval-2016 Task 4 on Sentiment Analysis on Twitter`` (http://alt.qcri.org/semeval2016/task4/). We focus on the subtask A which is coined as **message polarity classification task**. In this task, given a tweet, we need to predict whether the tweet is of **positive, negative or neutral sentiment**. We have 6,000, 1,999 and 20,632 tweets in train, validation, and test set respectively. We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/sentiment-twitter-2016-task4`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``. Some example tweets include:\n",
    "\n",
    "| class index | class name | tweet example |\n",
    "| ----------------- | ----------- |-------------|\n",
    "| 0  | Negative   | --MENTION-- --MENTION-- the reason i ask is because it may be the manufacturer's fault and they could help you |\n",
    "| 1  | Neutral | just ordered my ever tablet --MENTION-- surface pro --DIGIT-- ssd hopefully it works out for dev to replace my laptop |\n",
    "| 2  | Positive | dear --MENTION-- the newooffice for mac is great and all but no lync update c'mon |\n",
    "\n",
    "This tutorial assumes the data can be found at: `/content/drive/MyDrive/Colab Notebooks/sentiment-twitter-2016-task4`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spqTYO5pZXZ2",
    "outputId": "a5a0cccb-7eab-4006-f0cd-cbb567a76c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.4.2\n",
      "  Downloading transformers-4.4.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m669.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /Library/Python/3.7/site-packages (from transformers==4.4.2) (1.6.0)\n",
      "Requirement already satisfied: requests in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from transformers==4.4.2) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Python/3.7/site-packages (from transformers==4.4.2) (2020.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Library/Python/3.7/site-packages (from transformers==4.4.2) (0.10.1)\n",
      "Requirement already satisfied: packaging in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from transformers==4.4.2) (23.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from transformers==4.4.2) (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from transformers==4.4.2) (1.21.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Python/3.7/site-packages (from importlib-metadata->transformers==4.4.2) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from requests->transformers==4.4.2) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from requests->transformers==4.4.2) (2.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from requests->transformers==4.4.2) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from requests->transformers==4.4.2) (3.2)\n",
      "Requirement already satisfied: six in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from sacremoses->transformers==4.4.2) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/chiyuzhang/Library/Python/3.7/lib/python/site-packages (from sacremoses->transformers==4.4.2) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Library/Python/3.7/site-packages (from sacremoses->transformers==4.4.2) (0.14.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=018b4026d5022b8d85be574a56954b71ec221749905f04b01c6c0a649371edce\n",
      "  Stored in directory: /Users/chiyuzhang/Library/Caches/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: filelock, sacremoses, transformers\n",
      "\u001b[33m  WARNING: The script sacremoses is installed in '/Users/chiyuzhang/Library/Python/3.7/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/Users/chiyuzhang/Library/Python/3.7/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed filelock-3.9.0 sacremoses-0.0.53 transformers-4.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Library/Python/3.7/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mres_idx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mEOF_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c866bf1a0970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers==4.4.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install datasets==1.5.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sacrebleu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sentencepiece'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36msendline\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    576\u001b[0m         '''\n\u001b[1;32m    577\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_send_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_log_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.4.2\n",
    "!pip install datasets==1.5.0\n",
    "!pip install sacrebleu\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XWzJSPqZmGx"
   },
   "source": [
    "#### HuggingFace's run_seq2seq.py\n",
    "\n",
    "We will use T5 implementation provided by HuggingFace to finetune T5 for sentiment analysis. \n",
    "\n",
    "The **run_seq2seq.py** code provides implementation for fine-tuning T5 model and evaluating a trained T5 checkpoint. This tutorial assumes the code can be found at this path: `content/drive/MyDrive/Colab Notebooks/run_seq2seq.py`.\n",
    "\n",
    "Let's inspect some of the arguments the code takes in:\n",
    "- **task** - Name of the task. Set it to `translation_en_to_en`, as it doesn't matter for text classification.\n",
    "- **train_file** - Path to the training data file. The code accepts two format: jsonlines and csv. We will convert our sentiment dataset into jsonlines format.\n",
    "- **validation_file** - Path to the validation data file. The code accepts two format: jsonlines and csv. We will convert our sentiment dataset into jsonlines format.\n",
    "- **test_file** - Path to the test data file. The code accepts two format: jsonlines and csv. We will convert our sentiment dataset into jsonlines format.\n",
    "- **text_column** - Name of the column in the jsonlines dataset that corresponds to the input text (which is tweet in our setting). We will set it to \"input_text\".\n",
    "- **summary_column** - Name of the column in the jsonlines dataset that corresponds to the target text (which is sentiment label in our setting). We will set it to \"target_text\".\n",
    "- **model_name_or_path** - Model's shortcut name or path to pretrained model. For fine-tuning, we will set it to \"t5-base\". For evaluation, we will set it to path to the saved checkpoint.\n",
    "- **do_train** - Whether to run training or not. Set it during training.\n",
    "- **num_train_epochs** - Total number of training epochs to perform\n",
    "- **output_dir** - Output directory where the model predictions and checkpoints will be written.\n",
    "- **save_steps** - Number of updates steps before two checkpoint saves (default: 500)\n",
    "- **save_total_limit** - If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in **output_dir**\n",
    "- **predict_with_generate**, **do_predict** - Whether to generate the target text (sentiment label for our case) for validation and test or not.\n",
    "\n",
    "For extensive set of training arguments (e.g., learning rate, maximum steps, batch size), look [here](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9Y7McL8ozSo"
   },
   "source": [
    "\n",
    "\n",
    "#### Convert the dataset to jsonlines\n",
    "\n",
    "Our original sentiment data is in tsv format. For example, the first training sample in the dataset:\n",
    "\n",
    "dear \\<<\\<MENTION>>> the newooffice for mac is great and all but no lync update c'mon \\<TAB-SPACE>  1\n",
    "\n",
    "We can convert the sample to jsonlines format:\n",
    "\n",
    "{\"input_text\": \"dear <<<MENTION>>> the newooffice for mac is great and all but no lync update c'mon\", \"target_text\": \"positive\"}\n",
    "\n",
    "Note that the **input_text** field contains the original tweet (that corresponds to the value we use to set **text_column**) and the **target_text** field contains the sentiment label (that corresponds to the value we use to set **summary_column**).\n",
    "\n",
    "Let's convert the original dataset to jsonlines format now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YJ-E7tFsyppc"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "def convert_sentiment_dataset_to_text2text_format(original_folder, destination_folder):\n",
    "    # check if jsonlines directtory doesn't exist\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "    for src_file in [\"train.tsv\", \"dev.tsv\", \"test.tsv\"]:\n",
    "        t5_file = open(destination_folder + \"/\" + src_file.split(\".\")[0] + \".json\", \"w\")\n",
    "        for line in open(original_folder + \"/\" + src_file):\n",
    "            # read tsv line\n",
    "            tweet, sentiment = line.strip().split(\"\\t\")\n",
    "            # prepare json\n",
    "            t5_out = {}\n",
    "            t5_out[\"input_text\"] = tweet\n",
    "            if sentiment == \"0\":\n",
    "                t5_out[\"target_text\"] = \"negative\"\n",
    "            elif sentiment == \"1\":\n",
    "                t5_out[\"target_text\"] = \"neutral\"\n",
    "            else:\n",
    "                t5_out[\"target_text\"] = \"positive\"\n",
    "            # write json\n",
    "            t5_file.write(json.dumps(t5_out))\n",
    "            t5_file.write(\"\\n\")\n",
    "            t5_file.close()\n",
    "\n",
    "# assumes \"/content/drive/MyDrive/Colab Notebooks/sentiment-twitter-2016-task4\" contains original data\n",
    "# assumes \"/content/text2text-sentiment\" contains jsonlines data\n",
    "convert_sentiment_dataset_to_text2text_format(\"/content/drive/MyDrive/Colab Notebooks/sentiment-twitter-2016-task4\", \"text2text-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsytS7o-p7QM"
   },
   "source": [
    "\n",
    "#### Fine-tuning T5 model\n",
    "\n",
    "That's all the preparation needed. We can now use **run_seq2seq.py** script to finetune T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6t_HoP5Kqsa",
    "outputId": "cc3303a4-75bf-40f6-e9d9-b9b203c19954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-03 03:10:56.964951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "04/03/2021 03:10:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/03/2021 03:10:58 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/sentiment-ckpts', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr03_03-10-58_485fd034beda', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=5, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/sentiment-ckpts', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "04/03/2021 03:10:58 - WARNING - datasets.builder -   Using custom data configuration default-2ccb29283e2ad125\n",
      "04/03/2021 03:10:58 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/t5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/ab4e948915b067f5cb6e5105f6f85044fd717b133f43240db67899a8fc7b29a2.26934c75adf19ceac3c268b721ba353356b7609c45f5627550326f275a2163b4\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "04/03/2021 03:11:06 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-d119a78c40a8d72b.arrow\n",
      "04/03/2021 03:11:06 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-50ba29c637dfec9e.arrow\n",
      "04/03/2021 03:11:06 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-8b1a723d1720224b.arrow\n",
      "***** Running training *****\n",
      "  Num examples = 6000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2250\n",
      "{'loss': 0.8699, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n",
      " 22% 500/2250 [01:07<03:43,  7.82it/s]Saving model checkpoint to /content/sentiment-ckpts/checkpoint-500\n",
      "Configuration saved in /content/sentiment-ckpts/checkpoint-500/config.json\n",
      "Model weights saved in /content/sentiment-ckpts/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/sentiment-ckpts/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /content/sentiment-ckpts/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to /content/sentiment-ckpts/checkpoint-500/spiece.model\n",
      "{'loss': 0.396, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n",
      " 44% 1000/2250 [02:27<02:49,  7.39it/s]Saving model checkpoint to /content/sentiment-ckpts/checkpoint-1000\n",
      "Configuration saved in /content/sentiment-ckpts/checkpoint-1000/config.json\n",
      "Model weights saved in /content/sentiment-ckpts/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/sentiment-ckpts/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /content/sentiment-ckpts/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to /content/sentiment-ckpts/checkpoint-1000/spiece.model\n",
      "{'loss': 0.3642, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      " 67% 1500/2250 [03:50<01:37,  7.73it/s]Saving model checkpoint to /content/sentiment-ckpts/checkpoint-1500\n",
      "Configuration saved in /content/sentiment-ckpts/checkpoint-1500/config.json\n",
      "Model weights saved in /content/sentiment-ckpts/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /content/sentiment-ckpts/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /content/sentiment-ckpts/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to /content/sentiment-ckpts/checkpoint-1500/spiece.model\n",
      "{'loss': 0.3138, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n",
      " 89% 2000/2250 [05:10<00:33,  7.48it/s]Saving model checkpoint to /content/sentiment-ckpts/checkpoint-2000\n",
      "Configuration saved in /content/sentiment-ckpts/checkpoint-2000/config.json\n",
      "Model weights saved in /content/sentiment-ckpts/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /content/sentiment-ckpts/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /content/sentiment-ckpts/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to /content/sentiment-ckpts/checkpoint-2000/spiece.model\n",
      "100% 2250/2250 [05:58<00:00,  7.36it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 358.2668, 'train_samples_per_second': 6.28, 'epoch': 3.0}\n",
      "100% 2250/2250 [05:58<00:00,  6.28it/s]\n",
      "Saving model checkpoint to /content/sentiment-ckpts\n",
      "Configuration saved in /content/sentiment-ckpts/config.json\n",
      "Model weights saved in /content/sentiment-ckpts/pytorch_model.bin\n",
      "tokenizer config file saved in /content/sentiment-ckpts/tokenizer_config.json\n",
      "Special tokens file saved in /content/sentiment-ckpts/special_tokens_map.json\n",
      "Copy vocab file to /content/sentiment-ckpts/spiece.model\n",
      "04/03/2021 03:17:12 - INFO - __main__ -   ***** train metrics *****\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     epoch                      =      3.0\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     init_mem_cpu_alloc_delta   =      0MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     init_mem_cpu_peaked_delta  =      0MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     init_mem_gpu_alloc_delta   =    850MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     init_mem_gpu_peaked_delta  =      0MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_mem_cpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_mem_cpu_peaked_delta =     95MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_mem_gpu_alloc_delta  =   2578MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_mem_gpu_peaked_delta =    902MB\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_runtime              = 358.2668\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_samples              =     6000\n",
      "04/03/2021 03:17:12 - INFO - __main__ -     train_samples_per_second   =     6.28\n",
      "04/03/2021 03:17:12 - INFO - __main__ -   *** Test ***\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20632\n",
      "  Batch size = 8\n",
      "100% 2579/2579 [03:17<00:00, 12.63it/s]04/03/2021 03:20:32 - INFO - __main__ -   ***** test metrics *****\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_bleu                 =      0.0\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_gen_len              =      2.0\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_loss                 =   0.4419\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     40MB\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_mem_cpu_peaked_delta =     29MB\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_mem_gpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_mem_gpu_peaked_delta =    109MB\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_runtime              = 200.0434\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_samples              =    20632\n",
      "04/03/2021 03:20:32 - INFO - __main__ -     test_samples_per_second   =  103.138\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 8\n",
      "2829it [03:39, 13.55it/s]04/03/2021 03:20:52 - INFO - __main__ -   ***** dev metrics *****\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     dev_samples               =    1999\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     3MB\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     test_mem_cpu_peaked_delta =     3MB\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     test_mem_gpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     test_mem_gpu_peaked_delta =    71MB\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     val_bleu                  =     0.0\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     val_gen_len               =     2.0\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     val_loss                  =  0.4319\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     val_runtime               = 19.1964\n",
      "04/03/2021 03:20:52 - INFO - __main__ -     val_samples_per_second    = 104.134\n",
      "2829it [03:39, 12.86it/s]\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /content/sentiment-ckpts # ensure the directory to store the checkpoints is empty\n",
    "!python \"/content/drive/MyDrive/Colab Notebooks/run_seq2seq.py\" --task translation_en_to_en --text_column input_text --summary_column target_text --train_file /content/text2text-sentiment/train.json --validation_file /content/text2text-sentiment/dev.json --do_predict --predict_with_generate --test_file /content/text2text-sentiment/test.json --save_total_limit 5 --num_train_epochs 3 --output_dir /content/sentiment-ckpts --model_name_or_path t5-base --do_train --do_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKdocsdaqVkX"
   },
   "source": [
    "The above run saves a checkpoint every **save_steps** steps (default 500) and keeps only the latest **save_total_limit** checkpoints. The run doesn't print the validation performance so it's harder to monitor the training process. One trick is to save checkpoints frequently (within the hard disk space) and evaluate each checkpoint post training.\n",
    "\n",
    "#### Evaluating T5 model checkpoint\n",
    "\n",
    "Let's evaluate the latest checkpoints. We will use f1-micro as the metric to evaluate the quality of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OH0PeJoMN2da"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_checkpoint(prediction_dir, data_dir, metric=\"f1_micro\", do_val=True, do_test=True):\n",
    "  # compute validation performance\n",
    "    if do_val:\n",
    "    # read gold labels\n",
    "    gold_labels = []\n",
    "    for line in open(data_dir + \"/dev.json\"):\n",
    "        gold_labels.append(json.loads(line.strip())[\"target_text\"])\n",
    "    # read predicted labels\n",
    "    pred_labels = []\n",
    "    for line in open(prediction_dir + \"_val_preds_seq2seq.txt\"):\n",
    "        pred_labels.append(line.strip())\n",
    "    # compute metric\n",
    "    if metric == \"f1_micro\":\n",
    "        print(\"%s validation F1-micro: %.2f\"%(prediction_dir.split(\"/\")[-1], f1_score(gold_labels, pred_labels, average=\"micro\")))\n",
    "\n",
    "  # compute test performance\n",
    "    if do_test:\n",
    "    # read gold labels\n",
    "    gold_labels = []\n",
    "    for line in open(data_dir + \"/test.json\"):\n",
    "        gold_labels.append(json.loads(line.strip())[\"target_text\"])\n",
    "    # read predicted labels\n",
    "    pred_labels = []\n",
    "    for line in open(prediction_dir + \"_test_preds_seq2seq.txt\"):\n",
    "        pred_labels.append(line.strip())\n",
    "    # compute metric\n",
    "    if metric == \"f1_micro\":\n",
    "        print(\"%s test F1-micro: %.2f\"%(prediction_dir.split(\"/\")[-1], f1_score(gold_labels, pred_labels, average=\"micro\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjXhdo1-zFUK"
   },
   "source": [
    "Let's compute the validation and the testing performance of the checkpoint saved after 500th step: `/content/sentiment-ckpts/checkpoint-500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIaMDw70zFdJ",
    "outputId": "3ebd59f3-008c-4955-bd6c-e5867f4798ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-03 03:35:16.166289: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "04/03/2021 03:35:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/03/2021 03:35:17 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/sentiment-ckpts', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr03_03-35-17_485fd034beda', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/sentiment-ckpts', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "04/03/2021 03:35:17 - WARNING - datasets.builder -   Using custom data configuration default-2ccb29283e2ad125\n",
      "04/03/2021 03:35:17 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
      "loading configuration file /content/sentiment-ckpts/checkpoint-500/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file /content/sentiment-ckpts/checkpoint-500/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Didn't find file /content/sentiment-ckpts/checkpoint-500/tokenizer.json. We won't load it.\n",
      "Didn't find file /content/sentiment-ckpts/checkpoint-500/added_tokens.json. We won't load it.\n",
      "loading file /content/sentiment-ckpts/checkpoint-500/spiece.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file /content/sentiment-ckpts/checkpoint-500/special_tokens_map.json\n",
      "loading file /content/sentiment-ckpts/checkpoint-500/tokenizer_config.json\n",
      "loading weights file /content/sentiment-ckpts/checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/sentiment-ckpts/checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "100% 2/2 [00:00<00:00,  4.55ba/s]\n",
      "100% 21/21 [00:01<00:00, 20.96ba/s]\n",
      "04/03/2021 03:35:40 - INFO - __main__ -   *** Test ***\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20632\n",
      "  Batch size = 8\n",
      "100% 2579/2579 [03:14<00:00, 13.01it/s]04/03/2021 03:38:58 - INFO - __main__ -   ***** test metrics *****\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     init_mem_cpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     init_mem_cpu_peaked_delta =      0MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     init_mem_gpu_alloc_delta  =    850MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     init_mem_gpu_peaked_delta =      0MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_bleu                 =      0.0\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_gen_len              =      2.0\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_loss                 =   0.4948\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     40MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_mem_cpu_peaked_delta =     29MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_mem_gpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_mem_gpu_peaked_delta =    109MB\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_runtime              = 197.3177\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_samples              =    20632\n",
      "04/03/2021 03:38:58 - INFO - __main__ -     test_samples_per_second   =  104.562\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 8\n",
      "2829it [03:36, 13.64it/s]04/03/2021 03:39:18 - INFO - __main__ -   ***** dev metrics *****\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     dev_samples               =    1999\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     3MB\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     test_mem_cpu_peaked_delta =     3MB\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     test_mem_gpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     test_mem_gpu_peaked_delta =    72MB\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     val_bleu                  =     0.0\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     val_gen_len               =     2.0\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     val_loss                  =  0.4496\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     val_runtime               = 19.0458\n",
      "04/03/2021 03:39:18 - INFO - __main__ -     val_samples_per_second    = 104.958\n",
      "2829it [03:37, 13.03it/s]\n",
      "checkpoint-500 validation F1-micro: 0.57\n",
      "checkpoint-500 test F1-micro: 0.51\n"
     ]
    }
   ],
   "source": [
    "!python \"/content/drive/MyDrive/Colab Notebooks/run_seq2seq.py\" --model_name_or_path /content/sentiment-ckpts/checkpoint-500 --task translation_en_to_en --text_column input_text --summary_column target_text --train_file /content/text2text-sentiment/train.json --validation_file /content/text2text-sentiment/dev.json --test_file /content/text2text-sentiment/test.json --do_predict --predict_with_generate --output_dir /content/sentiment-ckpts --do_eval\n",
    "evaluate_checkpoint(\"/content/sentiment-ckpts/checkpoint-500\", \"/content/text2text-sentiment\", metric=\"f1_micro\", do_val=True, do_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EMyMldXss-y"
   },
   "source": [
    "Let's compute the validation and the testing performance of the checkpoint saved after 2,000th step: `/content/sentiment-ckpts/checkpoint-2000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDPlKDzSstD3",
    "outputId": "40cf42be-f3f7-4ec3-a68d-8790c529d64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-03 03:21:08.817697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "04/03/2021 03:21:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/03/2021 03:21:09 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/sentiment-ckpts', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr03_03-21-09_485fd034beda', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/sentiment-ckpts', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "04/03/2021 03:21:10 - WARNING - datasets.builder -   Using custom data configuration default-2ccb29283e2ad125\n",
      "04/03/2021 03:21:10 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
      "loading configuration file /content/sentiment-ckpts/checkpoint-2000/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file /content/sentiment-ckpts/checkpoint-2000/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Didn't find file /content/sentiment-ckpts/checkpoint-2000/tokenizer.json. We won't load it.\n",
      "Didn't find file /content/sentiment-ckpts/checkpoint-2000/added_tokens.json. We won't load it.\n",
      "loading file /content/sentiment-ckpts/checkpoint-2000/spiece.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file /content/sentiment-ckpts/checkpoint-2000/special_tokens_map.json\n",
      "loading file /content/sentiment-ckpts/checkpoint-2000/tokenizer_config.json\n",
      "loading weights file /content/sentiment-ckpts/checkpoint-2000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/sentiment-ckpts/checkpoint-2000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "100% 2/2 [00:00<00:00,  4.61ba/s]\n",
      "100% 21/21 [00:01<00:00, 20.55ba/s]\n",
      "04/03/2021 03:21:21 - INFO - __main__ -   *** Test ***\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20632\n",
      "  Batch size = 8\n",
      "100% 2579/2579 [03:12<00:00, 12.68it/s]04/03/2021 03:24:37 - INFO - __main__ -   ***** test metrics *****\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     init_mem_cpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     init_mem_cpu_peaked_delta =     0MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     init_mem_gpu_alloc_delta  =   850MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     init_mem_gpu_peaked_delta =     0MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_bleu                 =     0.0\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_gen_len              =     2.0\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_loss                 =  0.4279\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_mem_cpu_alloc_delta  =    40MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_mem_cpu_peaked_delta =    29MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_mem_gpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_mem_gpu_peaked_delta =   109MB\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_runtime              = 195.516\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_samples              =   20632\n",
      "04/03/2021 03:24:37 - INFO - __main__ -     test_samples_per_second   = 105.526\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 8\n",
      "2829it [03:34, 13.77it/s]04/03/2021 03:24:56 - INFO - __main__ -   ***** dev metrics *****\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     dev_samples               =    1999\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     3MB\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     test_mem_cpu_peaked_delta =     3MB\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     test_mem_gpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     test_mem_gpu_peaked_delta =    72MB\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     val_bleu                  =     0.0\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     val_gen_len               =     2.0\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     val_loss                  =  0.4288\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     val_runtime               = 18.8359\n",
      "04/03/2021 03:24:56 - INFO - __main__ -     val_samples_per_second    = 106.127\n",
      "2829it [03:35, 13.16it/s]\n",
      "checkpoint-2000 validation F1-micro: 0.62\n",
      "checkpoint-2000 test F1-micro: 0.62\n"
     ]
    }
   ],
   "source": [
    "!python \"/content/drive/MyDrive/Colab Notebooks/run_seq2seq.py\" --model_name_or_path /content/sentiment-ckpts/checkpoint-2000 --task translation_en_to_en --text_column input_text --summary_column target_text --train_file /content/text2text-sentiment/train.json --validation_file /content/text2text-sentiment/dev.json --test_file /content/text2text-sentiment/test.json --do_predict --predict_with_generate --output_dir /content/sentiment-ckpts --do_eval\n",
    "evaluate_checkpoint(\"/content/sentiment-ckpts/checkpoint-2000\", \"/content/text2text-sentiment\", metric=\"f1_micro\", do_val=True, do_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYyR7oIfstKM"
   },
   "source": [
    "Let's compute the validation and the testing performance of the final checkpoint: `/content/sentiment-ckpts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj_70zp6stPV",
    "outputId": "5433ef59-9fd2-4162-8dd2-de2a7f500826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-03 03:28:06.882525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "04/03/2021 03:28:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/03/2021 03:28:08 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/sentiment-ckpts', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Apr03_03-28-08_485fd034beda', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/sentiment-ckpts', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "04/03/2021 03:28:08 - WARNING - datasets.builder -   Using custom data configuration default-2ccb29283e2ad125\n",
      "04/03/2021 03:28:08 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-2ccb29283e2ad125/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
      "loading configuration file /content/sentiment-ckpts/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file /content/sentiment-ckpts/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Didn't find file /content/sentiment-ckpts/tokenizer.json. We won't load it.\n",
      "Didn't find file /content/sentiment-ckpts/added_tokens.json. We won't load it.\n",
      "loading file /content/sentiment-ckpts/spiece.model\n",
      "loading file None\n",
      "loading file None\n",
      "loading file /content/sentiment-ckpts/special_tokens_map.json\n",
      "loading file /content/sentiment-ckpts/tokenizer_config.json\n",
      "loading weights file /content/sentiment-ckpts/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/sentiment-ckpts.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "100% 2/2 [00:00<00:00,  4.50ba/s]\n",
      "100% 21/21 [00:01<00:00, 20.47ba/s]\n",
      "04/03/2021 03:28:19 - INFO - __main__ -   *** Test ***\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20632\n",
      "  Batch size = 8\n",
      "100% 2579/2579 [03:14<00:00, 12.76it/s]04/03/2021 03:31:36 - INFO - __main__ -   ***** test metrics *****\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     init_mem_cpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     init_mem_cpu_peaked_delta =      0MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     init_mem_gpu_alloc_delta  =    850MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     init_mem_gpu_peaked_delta =      0MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_bleu                 =      0.0\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_gen_len              =      2.0\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_loss                 =   0.4419\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     40MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_mem_cpu_peaked_delta =     29MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_mem_gpu_alloc_delta  =      0MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_mem_gpu_peaked_delta =    109MB\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_runtime              = 197.1435\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_samples              =    20632\n",
      "04/03/2021 03:31:36 - INFO - __main__ -     test_samples_per_second   =  104.655\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 8\n",
      "2829it [03:36, 13.70it/s]04/03/2021 03:31:56 - INFO - __main__ -   ***** dev metrics *****\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     dev_samples               =    1999\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     test_mem_cpu_alloc_delta  =     3MB\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     test_mem_cpu_peaked_delta =     3MB\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     test_mem_gpu_alloc_delta  =     0MB\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     test_mem_gpu_peaked_delta =    72MB\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     val_bleu                  =     0.0\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     val_gen_len               =     2.0\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     val_loss                  =  0.4319\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     val_runtime               = 19.0053\n",
      "04/03/2021 03:31:56 - INFO - __main__ -     val_samples_per_second    = 105.181\n",
      "2829it [03:36, 13.05it/s]\n",
      "t5-base validation F1-micro: 0.62\n",
      "t5-base test F1-micro: 0.61\n"
     ]
    }
   ],
   "source": [
    "!python \"/content/drive/MyDrive/Colab Notebooks/run_seq2seq.py\" --model_name_or_path /content/sentiment-ckpts --task translation_en_to_en --text_column input_text --summary_column target_text --train_file /content/text2text-sentiment/train.json --validation_file /content/text2text-sentiment/dev.json --test_file /content/text2text-sentiment/test.json --do_predict --predict_with_generate --output_dir /content/sentiment-ckpts --do_eval\n",
    "evaluate_checkpoint(\"/content/sentiment-ckpts/t5-base\", \"/content/text2text-sentiment\", metric=\"f1_micro\", do_val=True, do_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3gp6WeoYbtR"
   },
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "text_to_text_transfer_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
