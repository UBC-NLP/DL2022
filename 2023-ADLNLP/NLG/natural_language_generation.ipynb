{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHqeMTNSZxEj"
   },
   "source": [
    "## Generating Natural Language Text using GPT-2\n",
    "\n",
    "### Goal of this tutorial:\n",
    "- Learn about deterministic decoding techniques like greedy search and beam search.\n",
    "- Learn aboout stochastic decoding techniques like top-k and top-p (nucleus) sampling\n",
    "\n",
    "###  General:\n",
    "- This notebook was last tested on Python 3.6.4, PyTorch 0.4.0, **transformers 2.1.1** (note: tutorial might work only with this version of transformers)\n",
    "- This tutorial gives cuda out of memory error when run in Colab. Hence, it is recommended to run it on a local machine (CPU) with jupyter notebook\n",
    "\n",
    "### References\n",
    "To know more about the above-mentioned concepts, take a look at the following articles:\n",
    "1. GPT-1 Original Paper https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "2. GPT-2 Original Paper https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "3. Neural Text Degeneration with Unlikelihood Training https://arxiv.org/pdf/1908.04319.pdf (see Section 3)\n",
    "4. The Curious Case of Neural Text Degeneration https://arxiv.org/abs/1904.09751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUb589_xZxEk"
   },
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "### Generative Pre-Training - Version 2 (GPT-2)\n",
    "\n",
    "GPT-2 model is the state of the art model for natural language generation (NLG). GPT-2 model is based on **Transformer architecture** and uses language modeling (LM) objective, which works by maximizing the conditional probability of predicting a word in a text sequence (e.g., sentences) given the previous words of the target word. The model architecture is as shown below.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=12dvGzUzM_hM_LTAT8XfmJmsL8cu1Jyx3\" alt=\"GPT-2 model architecture\" title=\"GPT-2 model\" height=300 />\n",
    "\n",
    "Unlike BERT model that learns **bidirectional** representation, GPT-2 learns **unidirectional** representation (left to right) by conditioning on only left context in all layers. While BERT model is used primarily for solving **natural language understanding** tasks, GPT-2 model is primarily used for **natural language generation** (see next section).\n",
    "\n",
    "GPT-2 is trained on **WebText** that contains huge amounts of web pages. In this tutorial, we will stick to training GPT-2 on **Yelp dataset** which doesn't require much computing resources in comparison with training on WebText.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvHmRGqkgnAs"
   },
   "source": [
    "### Natural Language Generation\n",
    "Once we train a GPT-2 model, we might be keen to generate text by prompting the model with a seed text (e.g., beginning of a news article). One of the most popular generation from GPT-2 is:\n",
    "\n",
    "<img src=\"https://pbs.twimg.com/media/DzYpsJOU0AA1PO9.png\" alt=\"Famous unicorn example from GPT-2 Generations\" title=\"GPT2 - Generation\" height=600 width=550 />\n",
    "\n",
    "As you can observe, the resulting text seems to be **mostly coherent, grammatical, using long-term context and world knowledge**. We generally rely on either deterministic or stochastic decoding techniques to generate the text given a seed text and a trained language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iiG4ZWJgnup"
   },
   "source": [
    "### Deterministic Decoding Techniques\n",
    "\n",
    "There are two widely used decoding techniques, **greedy search** and **beam search**. The former is a special case of the latter. \n",
    "\n",
    "### Greedy search\n",
    "\n",
    "**Greedy search**, as the name suggests, outputs the token that received the highest probability at each time step. We have already used greedy search to generate translation from the decoder of the neural machine translation model. \n",
    "\n",
    "Let us see a sample generation using greedy search. During decoding, we first feed the seed text (prompt) as input to the model and predict the next most likely (argmax) word (first word in generation which is `the` in this example).\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1QBKIlCj3y8kZ27x5ojXcKANsOXga2qGf\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />\n",
    "\n",
    "Once we generate the first word, we append this word (`the`) to the input and predict the next most likely (argmax) word (second word in generation which is `scientist` in this example).\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=12eTq-gnjB9MM7BZb3IfXmWzBSM1LaXTL\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />\n",
    "\n",
    "We typically repeat the process until we have generated a certain number of words.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1feSME5YWr36MpZbfyWX0G1mdlXsv5j7G\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=370 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ad6KqM2csZTO"
   },
   "source": [
    "### Beam Search \n",
    "**On the other hand, beam search** maintains a fixed-size (which we call beam size) set of partially-decoded sequences, called hypotheses. At each time step, beam search creates new hypotheses by appending each token in the vocabulary to each existing hypothesis, scoring the resulting sequences. \n",
    "\n",
    "Let us see an example for beam search (with beam size of 2). In the first timestep, we feed the input (say `the unicorns spoke perfect english`) to the model, predict the top 2 words (based on the model probability for a word given the input) and store them in beam.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1gPCwzOUZOHbKl2gpHPtATr2YpLHUL-v_\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
    "\n",
    "**In the second timestep**, for each word in the beam, we append the word in the original input, feed to the model, keep track of all the resulting sequences (input along with generated word) along with their probabilites. We pick the top 2 resulting sequences and store them in beam.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1xuCidto4yjCBawiz9STcrUZ4UAVyANgr\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
    "\n",
    "**In the third timestep**, for each sequence in the beam, we append the sequence in the original input, feed to the model, keep track of all the resulting sequences (input along with generated word) along with their probabilites. We pick the top 2 resulting sequences and store them in beam. We repeat the process until we have generated the required number of words. The sequence in the beam with the highest probability is the generated sequence.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1hP8lfxkvVDN9q3Bu1WKWfKqjJMrHMVMo\" alt=\"Beam Search\" title=\"Beam Search\"  height=370 />\n",
    "\n",
    "In an open-domain generation, beam search generally leads to **degenerate text** with lot of repetitions (as seen in the below figure) compared to the admirable quality of the text decoded by stochastic method (see next section) like top-k sampling.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1Zcmch-8vxIhAImn7RKJObzz0DSw5tabi\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n",
    "\n",
    "Picture courtesy: https://arxiv.org/pdf/1904.09751.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoidIkjYgn4L"
   },
   "source": [
    "\n",
    "### Stochastic Decoding Techniques\n",
    "Stochastic decoding techniques sample from a model-dependent distribution at each step. The two successful techniques in this category are **top-k and nucleus (top-p)** sampling. To prevent sampling low probability tokens, a typical approach is to restrict sampling to a subset of the vocabulary at each step. \n",
    "\n",
    "The top-k sampler restricts sampling to the k most-probable tokens as shown below in an example.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1rjd8FjS_r1MmXwn8Hnbms7NgJJTa3x2j\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n",
    "\n",
    "\n",
    "Instead, **the nucleus (top-p) sampler** restricts sampling to the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1) as shown below in an example.\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1P3n2ygqxEQsYre-i_CvWQAOsqlzdho4Z\" alt=\"Beam Search\" title=\"Beam Search\"  height=270 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhe-DEByyDf6"
   },
   "source": [
    "## Implementation \n",
    "\n",
    "We will now perform the following:\n",
    "- Load the original GPT-2 language model (pretrained by the original authors)\n",
    "- Prompt the model with seed text and decode using \n",
    " - Greedy search\n",
    " - Top-k sampling\n",
    " - Top-p (or nucleus) sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Stf3dsAdoSA"
   },
   "source": [
    "As usual, let us start by loading the drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mU6BjQ8tif6a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCxd1-CiylpU"
   },
   "source": [
    "Install the transformers library (contains GPT-2 model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQRlA7hvzDus"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur-FXgpwzQpv"
   },
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oe5h1kdcZxEk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One place for all the imports\n",
    "'''\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
    "\n",
    "# import GPT-2 specific classes\n",
    "from transformers import (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AdamW, WarmupLinearSchedule)\n",
    "\n",
    "import logging\n",
    "logging.getLogger('transformers.tokenization_utils').setLevel(logging.ERROR)\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 123\n",
    "random.seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lD532G9i0GSg"
   },
   "source": [
    "Load GPT-2 config from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VJ2shXpZxEp"
   },
   "outputs": [],
   "source": [
    "cache_dir = \"/tmp\" # to store pretrained checkpoints\n",
    "\n",
    "# load config\n",
    "original_config = GPT2Config.from_pretrained(\"gpt2\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-Y0n_QZ1Ygs"
   },
   "source": [
    "Modify the config (hyperparameters) to train a toyish GPT-2 based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAGKyv4K1Yq-"
   },
   "outputs": [],
   "source": [
    "# construct a tutorial configuration file to setup a small version of GPT-2\n",
    "config = original_config\n",
    "config.n_layer = 2 # Number of hidden layers in the Transformer encoder. (default 12)\n",
    "config.n_embd = 60 # Dimensionality of the embeddings and hidden states. (default 768)\n",
    "config.n_head = 2 # Number of attention heads for each attention layer in the Transformer encoder. (default 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdMBmc1w1mdD"
   },
   "source": [
    "Instantiate the toyish GPT-2 model and GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gHlWNTw1m3w"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.to(device)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72VN9BPV17DN"
   },
   "source": [
    "Prepare the Yelp dataset used to train our LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhPl5aBSZxE8"
   },
   "source": [
    "## Text Generation with GPT-2 \n",
    "Now that we got a glimpse of training GPT-2 (toy version) from scratch, we can move to generating some interesting text from GPT-2 given some seed text.\n",
    "\n",
    "Let us create some seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v64O1YJvZxE-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "define some seed text to be given as input to GPT-2 model\n",
    "'''\n",
    "\n",
    "SEED_TEXT = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
    "researchers was the fact that the unicorns spoke perfect English. The\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgZMlNGnZxFG"
   },
   "source": [
    "We will use the original GPT-2 model pretrained on WebText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mt_CigxZxFG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "load model and tokenizer for GPT-2 (pretrained version)\n",
    "'''\n",
    "cache_dir = \"/tmp\" # to store pretrained checkpoints\n",
    "\n",
    "# load model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZ-rsh-28kOb"
   },
   "source": [
    "Set the number of words to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7SayHdJZxFJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "parameter for generation\n",
    "'''\n",
    "num_words_to_generate = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDC4gpTyZxFP"
   },
   "source": [
    "### Greedy Search\n",
    "\n",
    "Let us start with the most simplest but not very effective decoding technique, greedy search. Greedy search outputs the token that received the highest probability at each time step. \n",
    "\n",
    "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6zNZTMWePFK",
    "outputId": "20940245-6b92-448e-c354-1e4d41f1ca69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "[818, 257, 14702, 4917, 11, 11444, 5071, 257, 27638, 286]\n",
      "In a shocking finding, scientist discovered a herd of\n"
     ]
    }
   ],
   "source": [
    "# tokenize all the words in seed text using GPT-2 tokenizer\n",
    "seed_tokens = tokenizer.encode(SEED_TEXT, add_special_tokens=False)\n",
    "print(len(seed_tokens)) # there are 53 GPT-2 tokens in the seed text\n",
    "print(seed_tokens[0:10]) # token ids for first 10 tokens\n",
    "print(tokenizer.decode(seed_tokens[0:10])) # raw text based on first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVruSHJFfZIT"
   },
   "source": [
    "We will convert the token ids to tensor so that we can feed it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJlmUhV0gGYY",
    "outputId": "008eb208-ab31-4e19-9c7f-6a096773b59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "torch.Size([1, 53])\n"
     ]
    }
   ],
   "source": [
    " # create the tensor to store the model input (initially the input is just the seed text)\n",
    "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "print(generated)\n",
    "print(generated.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMBaa_DggbkJ"
   },
   "source": [
    "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_Z8WzjIgupg",
    "outputId": "f16350b6-673d-47e2-abf9-955ee5921af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53, 50257])\n"
     ]
    }
   ],
   "source": [
    "# prepare gpt-2 model input\n",
    "inputs = {'input_ids': generated}\n",
    "\n",
    "# feed input to the model\n",
    "outputs = model(**inputs)[0]\n",
    "\n",
    "print(outputs.size()) # [batch size, number of tokens in input, number of tokens in GPT-2 vocabulary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJVnztvvm8sP"
   },
   "source": [
    "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFeiceN_ocCN",
    "outputId": "ae570436-7b9a-4394-cb1f-bf5d02161ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# extract the next token logits (unnormalized probability distribution)\n",
    "next_token_logits = outputs[:, -1, :]\n",
    "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBdt2g7aocgK"
   },
   "source": [
    "**In greedy sampling, we choose the word with the highest probability as the word to be generated.**\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1QBKIlCj3y8kZ27x5ojXcKANsOXga2qGf\" alt=\"Greedy Search\" title=\"Greedy Search\"  height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Z2cxOfun9xu",
    "outputId": "a6c3d2b7-92d0-4359-8919-11d2a94cd144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4837]])\n"
     ]
    }
   ],
   "source": [
    "# find the token with the highest probability (argmax)\n",
    "next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "print(next_token) # id for the predicted token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uI_Vby5-pQqD"
   },
   "source": [
    "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yjakss0iqgf7",
    "outputId": "584e9b11-bed5-462f-b16d-bf10c312e3ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "after appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383,  4837]])\n",
      "our text (seed text + generated text) so far...\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The researchers\n"
     ]
    }
   ],
   "source": [
    "print('before appending the new generated token...')\n",
    "print(generated)\n",
    "# add the generated token to the input\n",
    "generated = torch.cat((generated, next_token), dim=1)\n",
    "print('after appending the new generated token...')\n",
    "print(generated)\n",
    "print('our text (seed text + generated text) so far...')\n",
    "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mD9MaGmDsXnT"
   },
   "source": [
    "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
    "\n",
    "And the full fledged code for greedy search will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO6hMB1BZxFS",
    "outputId": "82eb0df0-6922-4f4f-aafe-48689242f3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy search's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The\n",
      "greedy search's continuation text:  researchers say that the unicorns were able to communicate with each other through their\n",
      "\n",
      "speech, and that they were able to communicate with each other through their eyes.\n",
      "\n",
      "The researchers say that the unicorns were able to communicate with each other through their eyes.\n",
      "\n",
      "The researchers say that the unicorns were able to communicate with each other through\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(seed_text):  \n",
    "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
    "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
    "  \n",
    "  # create the tensor to store the model input (initially the input is just the seed text)\n",
    "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for _ in range(num_words_to_generate): # run over number of word to generate\n",
    "      # prepare gpt-2 model input\n",
    "      inputs = {'input_ids': generated}\n",
    "\n",
    "      # feed input to the model\n",
    "      outputs = model(**inputs)[0]\n",
    "\n",
    "      # extract the next token logits (unormalized probability distribution)\n",
    "      next_token_logits = outputs[:, -1, :]\n",
    "\n",
    "      # find the token with highest probability (argmax)\n",
    "      next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "      # add the generated token to the input\n",
    "      generated = torch.cat((generated, next_token), dim=1)\n",
    "  \n",
    "  # convert the model generation from token ids to raw text\n",
    "  generated = generated[:, len(seed_tokens):].tolist() # seed tokens are already in raw form\n",
    "  for g in generated: # for every generated token\n",
    "    text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
    "\n",
    "  return text\n",
    "\n",
    "print(\"greedy search's seed text = %s\"%SEED_TEXT)\n",
    "print(\"greedy search's continuation text: %s\"%greedy_search(SEED_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kZQRI05ZxFV"
   },
   "source": [
    "Greedy search clearly suffers from repetition problem. In general, greedy search doesn't work well for open-domain generation.\n",
    "\n",
    "To overcome the repetition problem and also encourage diverse text, we will be exploring the stochastic methods that works by preventing sampling of low probability tokens. A typical approach is to restrict sampling to a subset of the vocabulary at each step.\n",
    "\n",
    "### Top-k sampling\n",
    "The top-k sampler restricts sampling to the k most-probable tokens. This was the decoding strategy used by the original GPT-2 paper.\n",
    "\n",
    "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCTey_hftTDE",
    "outputId": "8ca8f5f6-abba-4bda-a46d-17e49a5199be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "[818, 257, 14702, 4917, 11, 11444, 5071, 257, 27638, 286]\n",
      "In a shocking finding, scientist discovered a herd of\n"
     ]
    }
   ],
   "source": [
    "# tokenize all the words in seed text using GPT-2 tokenizer\n",
    "seed_tokens = tokenizer.encode(SEED_TEXT, add_special_tokens=False)\n",
    "print(len(seed_tokens)) # there are 53 GPT-2 tokens in the seed text\n",
    "print(seed_tokens[0:10]) # token ids for first 10 tokens\n",
    "print(tokenizer.decode(seed_tokens[0:10])) # raw text based on first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbTz7Ag9tV1q"
   },
   "source": [
    "We will convert the token ids to tensor so that we can feed it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPBNxqBetbKB",
    "outputId": "ff7b9517-11e9-489f-ec83-0a26df4f39d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "torch.Size([1, 53])\n"
     ]
    }
   ],
   "source": [
    "# create the tensor to store the model input (initially the input is just the seed text)\n",
    "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "print(generated)\n",
    "print(generated.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0U_x-TMdtehf"
   },
   "source": [
    "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YBa3yeDtgPD",
    "outputId": "d7c58647-8ae8-4e20-88b9-e214630c87ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53, 50257])\n"
     ]
    }
   ],
   "source": [
    "# prepare gpt-2 model input\n",
    "inputs = {'input_ids': generated}\n",
    "\n",
    "# feed input to the model\n",
    "outputs = model(**inputs)[0]\n",
    "\n",
    "print(outputs.size()) # batch size x number of tokens in input x number of tokens in GPT-2 vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhtJihUTtkQb"
   },
   "source": [
    "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBgdORtxto1g",
    "outputId": "e43f2c46-dd24-4d02-f1a4-d7209ff1b42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# extract the next token logits (unnormalized probability distribution)\n",
    "next_token_logits = outputs[:, -1, :]\n",
    "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krRPfV-4uHpl"
   },
   "source": [
    "**In top-k sampling, we sample the word from the k most probable tokens to generate the next word.**\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1rjd8FjS_r1MmXwn8Hnbms7NgJJTa3x2j\" alt=\"Top-k Sampling\" title=\"Top-k Sampling\"  height=270 />\n",
    "\n",
    "So we first remove all tokens with a probability less than the last token of the top-k list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlYfE20svL3A",
    "outputId": "8a25f731-c9df-4dd3-8738-0d08ed6d78f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Remove all tokens with a probability less than the last token of the top-k list\n",
    "indices_to_remove = next_token_logits < torch.topk(next_token_logits, 40)[0][..., -1, None] # top_k = 40\n",
    "next_token_logits[indices_to_remove] = -float('Inf') # substitute negative infinity so that those words would never be sampled\n",
    "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dba24mOdvgjV"
   },
   "source": [
    "Now we can sample from top-k probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKfDJctivlMu",
    "outputId": "7ea793a7-d2ac-49b2-81ec-e1f790410ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[717]])\n"
     ]
    }
   ],
   "source": [
    "# Sample from top-k probability distribution\n",
    "next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "print(next_token) # id for the predicted token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PetbmddmwKt4"
   },
   "source": [
    "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JztuW4C-wL8v",
    "outputId": "80fd0b3f-25da-4eed-d4b4-c73021d2f175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "after appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383,   717]])\n",
      "our text (seed text + generated text) so far...\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The first\n"
     ]
    }
   ],
   "source": [
    "print('before appending the new generated token...')\n",
    "print(generated)\n",
    "# add the generated token to the input\n",
    "generated = torch.cat((generated, next_token), dim=1)\n",
    "print('after appending the new generated token...')\n",
    "print(generated)\n",
    "print('our text (seed text + generated text) so far...')\n",
    "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evExzQGZxSDg"
   },
   "source": [
    "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
    "\n",
    "And the full fledged code for top-k sampling will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJsOzHtwZxFV",
    "outputId": "84aecea9-1710-411d-d25e-8178f0edb738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-k sampling's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The\n",
      "top-k sampling's continuation text =  only explanation for this is that unicorns have learned what many people\n",
      "will most likely never know, that language is a sign\n",
      "or an instrument.\n",
      "\n",
      "It has always seemed like the people living in the valley would never be able to recognize unicorns. However it turns\n",
      "\n",
      "out that they have learned a new way to talk to one another\n"
     ]
    }
   ],
   "source": [
    "def topk_sampling(seed_text, top_k=100, filter_value=-float('Inf')): \n",
    "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
    "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
    "\n",
    "  # create the tensor to store the model input (initially the input is just the seed text)\n",
    "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words_to_generate): # run over number of word to generate\n",
    "            # prepare gpt-2 model input\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            # feed input to the model\n",
    "            outputs = model(**inputs)[0]\n",
    "\n",
    "            # extract the next token logits (unormalized probability distribution)\n",
    "            next_token_logits = outputs[:, -1, :] \n",
    "\n",
    "            # Safety check just in case number of words in the vocabulary is less than top_k, \n",
    "            # we need to set top_k to number of words in the vocabulary\n",
    "            top_k = min(top_k, next_token_logits.size(-1))  \n",
    "\n",
    "            # Remove all tokens with a probability less than the last token of the top-k list\n",
    "            indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "            next_token_logits[indices_to_remove] = filter_value # substitute negative infinity so that those words would never be sampled\n",
    "\n",
    "            # Sample from top-k probability distribution\n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "\n",
    "            # add the generated token to the input\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "  # convert the model generation from token ids to raw text\n",
    "    generated = generated[:, len(seed_tokens):].tolist() # seed tokens are already in raw form\n",
    "    for g in generated: # for every generated token\n",
    "        text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return text\n",
    " \n",
    "print(\"top-k sampling's seed text = %s\"%SEED_TEXT)\n",
    "print(\"top-k sampling's continuation text = %s\"%topk_sampling(SEED_TEXT, top_k=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3lDVDUHZxFZ"
   },
   "source": [
    "### Top-p (or nucleus) sampling\n",
    "\n",
    "On the other hand, the nucleus (top-p) sampler restricts sampling to the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1).\n",
    "\n",
    "We will first tokenize the seed text (`In a shocking finding, scientist ...`) and extract the GPT-2 tokens using GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5EnQdZC0GEh",
    "outputId": "f59c22ae-7654-4c3c-a74e-cd4350eba9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "torch.Size([1, 53])\n"
     ]
    }
   ],
   "source": [
    "# create the tensor to store the model input (initially the input is just the seed text)\n",
    "generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "print(generated)\n",
    "print(generated.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzCnMoKv0IXG"
   },
   "source": [
    "We will now feed the tensor containing token ids to the GPT-2 model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pX4ZSFoi0Igc",
    "outputId": "8ecaa23d-c7ea-47e1-ec00-eec5bf40cd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53, 50257])\n"
     ]
    }
   ],
   "source": [
    "# prepare gpt-2 model input\n",
    "inputs = {'input_ids': generated}\n",
    "\n",
    "# feed input to the model\n",
    "outputs = model(**inputs)[0]\n",
    "\n",
    "print(outputs.size()) # batch size x number of tokens in input x number of tokens in GPT-2 vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW7G6o6k0L0t"
   },
   "source": [
    "The `outputs` tensor contains the logits (unnormalized probability distribution over words in GPT-2 vocabulary) for each token in the input and we will extract the logits from the last token (53rd token) to decide the token to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzhnxJsS0N4R",
    "outputId": "48115515-30fc-4fda-fec0-01da9d01bf73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# extract the next token logits (unnormalized probability distribution)\n",
    "next_token_logits = outputs[:, -1, :]\n",
    "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7fjM2ZC0SzG"
   },
   "source": [
    "**In top-p sampling, we sample from the smallest set of tokens with total mass above a threshold p (which is a continuous value that ranges between 0 and 1) as shown below in an example.**\n",
    "\n",
    "<img src=\"\n",
    "https://drive.google.com/uc?id=1P3n2ygqxEQsYre-i_CvWQAOsqlzdho4Z\" alt=\"top-p sampling\" title=\"top-p sampling\"  height=270 />\n",
    "\n",
    "So we first compute the cummulative probability (the last column in the right most table in the figure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZnyYOvs1nig",
    "outputId": "f7b65f25-62b3-475d-b930-dc123c98e73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# compute the cummulative probability\n",
    "sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True) # sort words based on probability\n",
    "cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "print(cumulative_probs.size()) # 1 x number of tokens in GPT-2 vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNGZ5pgV2a1n"
   },
   "source": [
    "And the we remove tokens with cumulative probability above the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIl1SpAz2fbV",
    "outputId": "5ade3454-a77e-4b33-a85a-499b8dc0348f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# Remove tokens with cumulative probability above the threshold\n",
    "sorted_indices_to_remove = cumulative_probs > 0.9 # top-p=0.9\n",
    "# Shift the indices to the right to keep also the first token above the threshold\n",
    "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "sorted_indices_to_remove[..., 0] = 0\n",
    "# scatter sorted tensors to original indexing\n",
    "indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "next_token_logits[indices_to_remove] = -float('Inf') # substitute negative infinity so that those words would never be sampled\n",
    "print(next_token_logits.size()) # unnormalized probability distribution for next word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrr2zlWH3YTK"
   },
   "source": [
    "Now we can sample from top-p probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zszufgAq3c0g",
    "outputId": "8efe81f4-03f5-4e0f-8354-37a4afdb8990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7185]])\n"
     ]
    }
   ],
   "source": [
    "# Sample from top-p probability distribution\n",
    "next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "print(next_token) # id for the predicted token     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0t_dZW33i8o"
   },
   "source": [
    "Now we can append this word to the input tensor (`generated`) and print out the generated text so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NEYjmBj3nM7",
    "outputId": "71137f11-13cb-4c29-aafc-87cf0699fb11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383]])\n",
      "after appending the new generated token...\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,   220,   198,  3866,\n",
      "          8647, 31286,  1850, 19272,    11,   287,   262,   843,   274, 21124,\n",
      "            13,  3412,   517,  6452,   284,   262,   198,   260,   325,   283,\n",
      "          3533,   373,   262,  1109,   326,   262, 28000, 19942,  5158,  2818,\n",
      "          3594,    13,   383,  7185]])\n",
      "our text (seed text + generated text) so far...\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The creature\n"
     ]
    }
   ],
   "source": [
    "print('before appending the new generated token...')\n",
    "print(generated)\n",
    "# add the generated token to the input\n",
    "generated = torch.cat((generated, next_token), dim=1)\n",
    "print('after appending the new generated token...')\n",
    "print(generated)\n",
    "print('our text (seed text + generated text) so far...')\n",
    "print(tokenizer.decode(generated.squeeze().tolist(), clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R25nS4xb3pLc"
   },
   "source": [
    "We can repeat the process with the new `generated` (input) tensor and predict the rest of the words you want to generate.\n",
    "\n",
    "And the full fledged code for top-p sampling will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iguauNBhZxFb",
    "outputId": "f570cd08-d9a0-433a-b188-ad22f42ce375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nucleus (top-p) sampling's seed text = In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
      "previously unexplored valley, in the Andes Mountains. Even more surprising to the\n",
      "researchers was the fact that the unicorns spoke perfect English. The\n",
      "nucleus (top-p) sampling's continuation text =  unicorns did not give birth to male sons, they took on a different and more\n",
      "\n",
      "apparently homosexual type of behaviour in their \"voyeuristic appearance\". The researchers found that the unicorns gave birth to a\n",
      "\n",
      "child with normal whisker length and hematocrit rather than average whisker length. However, over time the unic\n"
     ]
    }
   ],
   "source": [
    "def top_p_sampling(seed_text, top_p=0.9, filter_value=-float('Inf')):  \n",
    "  # tokenize all the words in seed text using GPT-2 tokenizer\n",
    "  seed_tokens = tokenizer.encode(seed_text, add_special_tokens=False)\n",
    "  \n",
    "  # create the tensor to store the model input (initially the input is just the seed text)\n",
    "  generated = torch.tensor(seed_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for _ in range(num_words_to_generate):\n",
    "      # prepare gpt-2 model input\n",
    "      inputs = {'input_ids': generated}\n",
    "\n",
    "      # feed input to the model\n",
    "      outputs = model(**inputs)[0]\n",
    "\n",
    "      # extract the next token logits (unormalized probability distribution)\n",
    "      next_token_logits = outputs[:, -1, :]\n",
    "      \n",
    "      # compute the cummulative probability\n",
    "      sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "      cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "      # Remove tokens with cumulative probability above the threshold\n",
    "      sorted_indices_to_remove = cumulative_probs > top_p\n",
    "      # Shift the indices to the right to keep also the first token above the threshold\n",
    "      sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "      sorted_indices_to_remove[..., 0] = 0\n",
    "      # scatter sorted tensors to original indexing\n",
    "      indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "      next_token_logits[indices_to_remove] = filter_value # substitute negative infinity so that those words would never be sampled\n",
    "\n",
    "      # Sample from top-p probability distribution\n",
    "      next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "      \n",
    "      # add the generated token to the input\n",
    "      generated = torch.cat((generated, next_token), dim=1)\n",
    "  \n",
    "  # convert the model generation from token ids to raw text\n",
    "  generated = generated[:, len(seed_tokens):].tolist() \n",
    "  for g in generated:\n",
    "    text = tokenizer.decode(g, clean_up_tokenization_spaces=True)\n",
    "  \n",
    "  return text\n",
    "\n",
    "print(\"nucleus (top-p) sampling's seed text = %s\"%SEED_TEXT)\n",
    "print(\"nucleus (top-p) sampling's continuation text = %s\"%top_p_sampling(SEED_TEXT, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXOlRUtYdzBF"
   },
   "source": [
    "That's it!\n",
    "\n",
    "If you're curious, try out the online GPT-2 demo: https://transformer.huggingface.co/doc/gpt2-large"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "natural_language_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
