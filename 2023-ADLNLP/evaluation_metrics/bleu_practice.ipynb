{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 BLEU in the face\n",
    "\n",
    "BLEU can be a little counter-intuitive, it's not the only metric used to evaluate translations (ROUGE, METEOR etc.), but it's by far the most common. Some issues with it is that it's not really a percentage (even though it's out of 100) and is more useful to think about in terms of relative comparison to other translations in the same domain (rather than % of a sentence translated \"correctly\"). The goal of this problem is to walk you through evaluating a couple different strings to see how differences in the translation impact the final BLEU score, so that you have a more intuitive sense of what the metric means.\n",
    "\n",
    "\n",
    "#### Sidebar on BLEU\n",
    "BLEU in some domains might be very low, but still potentially useful. State of the art translation results in, for instance, low-resource spoken language translation might be single digits to at most low 20s, what's important is that the score is compared to other scores in the same domain (under the same conditions).\n",
    "\n",
    "Finally we'll just be using a single reference sentence for this example, in some cases you might have access to corpora with multiple references, this generally will lead to overall higher calculated BLEU score (but may not actually reflect an improvement in translation quality per se).\n",
    "\n",
    "### Sentence BLEU calculation\n",
    "\n",
    "For the following sentences follow the steps to compute the Sentence BLEU score by hand.  \n",
    "```\n",
    "Candidate Sentence 1: \"The the the the the the the the the the the the the the the the\" [Length: 16]\n",
    "\n",
    "Candidate Sentence 2: \"The north wind and sun are awakening, which is even stronger.\" [Length: 11]\n",
    "\n",
    "Candidate Sentence 3: \"Sun had a quarrel\" [Length: 4]\n",
    "\n",
    "Reference Sentence: \"The North Wind and the Sun had a quarrel about which of them was the stronger.\" [Length: 16]\n",
    "```\n",
    "\n",
    "#### For all of the below problems fill in the appropriate section with the fraction or decimal that corresponds to the calculated (partial) BLEU score based on the different components that make up BLEU\n",
    "\n",
    "\n",
    "## Note Solutions Assume 'non-case sensitive' setting (this is generally how MT is evaluated, but not necessarily true of other tasks), credit given for 'case sensitive' solutions as well, with the implication being the Sentence 2 is better in the non-case sensitive setting, but Sentence 3 becomes better once you consider case.\n",
    "\n",
    "### 3.1 Unigram Precision\n",
    "\n",
    "Calculate the unigram precision (e.g. how each unigram appears in the reference sentence, divided by the length of the candidate sentence) for each of these sentences. (Hint: easy to just count words that are NOT in the reference) LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Clipped Unigram Precision\n",
    "\n",
    "Calculate the unigram precision (as before) but clip it so that unigrams can only be counted up to the max number of times they occur in the reference sentence. LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clipped n-gram Precision\n",
    "\n",
    "As our clipped unigram precision before, but this time calculate the clipped 2,3,and 4-gram precisions of our sentences. (HINT: There are [length - (n-1)] n-grams in a sentence.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Brevity Penalty \n",
    "\n",
    "BLEU gives a brevity penalty (BP) based on the length of the candidate sentence ($c$) compared to the reference sentence ($r$) as follows: for $c \\geq r$ then $BP = 1$ for $c < r$ then $BP = exp(1-r/c)$\n",
    "\n",
    "Compute the Brevity Penalty for our candidate sentences.  (Can leave as decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Final BLEU \n",
    "\n",
    "The final BLEU score is then calculated as such: \n",
    "$BLEU = BP * exp(\\sum_{n=1}^N w_n log p_n)$   where $w_n$ is the weight for each of the n-grams (in our case use .25 each) and $p_n$ is the clipped precision for each of the n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
