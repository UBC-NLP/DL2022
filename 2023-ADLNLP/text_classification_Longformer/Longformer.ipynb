{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d832dc6f-9b2d-4268-bb7e-5b7a38d84d6a",
   "metadata": {},
   "source": [
    "## Longformer [[Beltagy et al.]](https://arxiv.org/abs/2004.05150)\n",
    "\n",
    "Longformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only attend \"locally\" to each other meaning that each token attends to its w/2 previous tokens and w/2 succeeding tokens with w being the window length. To increase the receptive field, the authors also applied dilation to the local window so they can increase the size of w without incurring in additional memory costs. This allows us to attend more than 512 tokens. </br>\n",
    "In this tutorial, we will see an example of how to use Longformer for text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dfdc4-7513-4ea5-9aaa-293d0cb7c17c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"Images/longformer.PNG\"  width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d61930-c28a-40ec-aedd-912807c896ff",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2851941-c334-4f50-8c38-ef640eb6e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, sys, regex\n",
    "import torch\n",
    "#import GPUtil\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "##----------------------------------------------------\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, LongformerConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import datasets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b1151f-4a56-4eaa-b9a0-71cc104396af",
   "metadata": {},
   "source": [
    "### Function for Tokenizing Train & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6ab0a9-bd61-4d04-a047-2c3fb89b9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, lab2ind, text_col_1 = 'sentence', text_col_2 = None, label_col = 'labels'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_col_1 = self.data[text_col_1]\n",
    "        if(text_col_2 is None):\n",
    "            self.text_col_2 = None\n",
    "        else:\n",
    "            self.text_col_2 = self.data[text_col_2]\n",
    "        self.labels = self.data[label_col]\n",
    "        self.max_len = max_len\n",
    "        self.lab2ind = lab2ind\n",
    "        \n",
    "        self.isPair = True\n",
    "        if(self.text_col_2 is None):\n",
    "            self.isPair = False\n",
    "            self.text_col_2 = self.data[text_col_1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_col_1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_1 = str(self.text_col_1[index])     \n",
    "        text_2 = str(self.text_col_2[index]) \n",
    "        \n",
    "        label = self.labels[index]\n",
    "        label = self.lab2ind[label]\n",
    "        try:\n",
    "            label = self.lab2ind[label]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if(self.isPair):\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "            [text_1, text_2],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        else:\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "            [text_1],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            #return_token_type_ids=False, !!!!!!!!!\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        if(self.isPair):\n",
    "            dic = {\n",
    "            'ids': torch.tensor(inputs.input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs.attention_mask, dtype=torch.long),\n",
    "            'token': torch.tensor(inputs.token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        else:\n",
    "            dic = {\n",
    "            'ids': torch.tensor(inputs.input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs.attention_mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420975e-3e97-4e2a-a15a-d5b79228e024",
   "metadata": {},
   "source": [
    "### Function for Encoding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918df8df-7fde-4957-a3cf-0702693ae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for data preparation\n",
    "def regular_encode(file_path, tokenizer, lab2ind, shuffle=True, num_workers = 1, batch_size=64, maxlen = 32, mode = 'train', text_col_1 = 'sentence', text_col_2 = None, label_col = 'labels'):\n",
    "    \n",
    "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
    "    delimiter = None\n",
    "    if(str(file_path).endswith('tsv')):\n",
    "        delimiter = '\\t'\n",
    "    if mode == 'train':\n",
    "        # Use pandas to load dataset\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind, text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    \n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    elif mode == 'evaluate':\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind, text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    else:\n",
    "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
    "\n",
    "        return\n",
    "        \n",
    "    print(\"{} Dataset: {}\".format(file_path, df.shape))\n",
    "    \n",
    "    dataset_params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers}\n",
    "\n",
    "    batch_data_loader = DataLoader(custom_set, **dataset_params)\n",
    "    \n",
    "    return batch_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0ce1d-0542-44ea-a4bc-c690c18acc8b",
   "metadata": {},
   "source": [
    "### Define Longformer Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b105cf50-d851-41bd-b292-8972a8ae7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20,\n",
      "    20\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer and model and define length of the text sequence\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 128)\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                           gradient_checkpointing=False,\n",
    "                                                           attention_window = 20, num_labels=2)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4eb456-5512-47ae-ac9d-35c6affeb698",
   "metadata": {},
   "source": [
    "### Define Train Function for Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bb234b-96a9-40c7-a7d8-a20be9bc3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler, isPair = False):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for _, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        \n",
    "        input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        try:\n",
    "            token_type_ids = batch['token'].to(device, dtype=torch.long)\n",
    "        except:\n",
    "            token_type_ids = None\n",
    "        labels = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        num_sent = input_ids.shape[1]\n",
    "\n",
    "        if (len(input_ids.shape) == 3):\n",
    "            input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "            attention_mask = attention_mask.view((-1, input_ids.size(-1)))\n",
    "        if (token_type_ids and (len(token_type_ids.shape) == 3) and (isPair==True)):\n",
    "            token_type_ids = token_type_ids.view((-1, input_ids.size(-1)))\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels)\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "\n",
    "        if torch.cuda.device_count() == 1:\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.cpu().item()\n",
    "\n",
    "        elif torch.cuda.device_count() > 1:\n",
    "            loss.mean().backward()\n",
    "            epoch_loss += loss.mean().cpu().item()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # free GPU memory\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2c16f-0008-4c39-ab61-7a7c1c8de4b5",
   "metadata": {},
   "source": [
    "### Define Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd3cef1-d3c1-4948-85a7-fa81c3184b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, metric, is_regression = False, isPair = False):\n",
    "    AvgRec=0.00\n",
    "    Fpn=0.00\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator, 0):\n",
    "        # Add batch to GPU\n",
    "            input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            try:\n",
    "                token_type_ids = batch['token'].to(device, dtype=torch.long)\n",
    "            except:\n",
    "                token_type_ids = None\n",
    "            labels = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            if (len(input_ids.shape) == 3):\n",
    "                input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "                attention_mask = attention_mask.view((-1, input_ids.size(-1)))\n",
    "            if (token_type_ids and (len(token_type_ids.shape) == 3) and (isPair==True)):\n",
    "                token_type_ids = token_type_ids.view((-1, input_ids.size(-1)))\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # delete used variables to free GPU memory\n",
    "            del batch, input_ids, attention_mask, token_type_ids\n",
    "\n",
    "            if torch.cuda.device_count() == 1:\n",
    "                epoch_loss += loss.cpu().item()\n",
    "            else:\n",
    "                epoch_loss += loss.sum().cpu().item()\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(logits.cpu() if is_regression else predicted)\n",
    "            all_label.extend(labels.cpu())\n",
    "\n",
    "   \n",
    "    if(is_regression):\n",
    "        result =  {\"mse\": (np.array((all_pred - all_label) ** 2)).mean().item()}\n",
    "    else:\n",
    "        result = metric(predictions=all_pred, references=all_label)\n",
    "    return epoch_loss/len(iterator), result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99329e49-97f5-4e61-961f-c5919ead769e",
   "metadata": {},
   "source": [
    "### Create Optimizer and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061cdaa2-6ff6-4969-bca3-b1a8b2504172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(total_params, num_training_steps, warmup_steps, weight_decay, learning_rate, is_constant_lr):\n",
    "    \"\"\"\n",
    "    Setup the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in total_params if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in total_params if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    if is_constant_lr == True:\n",
    "    \tlr_scheduler = get_constant_schedule(optimizer)\n",
    "    else:\n",
    "\t    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "\t        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "\t    )\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3483e199-582d-4648-b43a-08235131e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean().item()\n",
    "\n",
    "def accuracy(predictions, references):\n",
    "    acc = accuracy_score(references, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23eed6-1351-4ab0-8378-5baabe801a2c",
   "metadata": {},
   "source": [
    "### Finetuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3781011-48c8-469d-a4e0-16b65a7bd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(model, tokenizer, config, seed):\n",
    "    set_seed(seed)\n",
    "    #---------------------------------------\n",
    "\n",
    "    task_name = config[\"task_name\"]\n",
    "    text_col_1 = config[\"text_column_1\"]\n",
    "    isPair = False ## Not Pair Sentence Classification\n",
    "    try:\n",
    "        text_col_2 = config[\"text_column_2\"]\n",
    "        if(text_col_2 is not None):\n",
    "            isPair = True\n",
    "    except: \n",
    "        text_col_2 = None\n",
    "    label_col = config[\"label_column\"]\n",
    "\n",
    "    train_file = os.path.join(config[\"data_dir\"], config[\"train_file\"])\n",
    "\n",
    "\n",
    "    try:\n",
    "        is_constant_lr = config[\"is_constant_lr\"]\n",
    "    except: \n",
    "        is_constant_lr = False\n",
    "\n",
    "\n",
    "        \n",
    "    dev_file = os.path.join(config[\"data_dir\"], config[\"dev_file\"])\n",
    "    test_file = os.path.join(config[\"data_dir\"], config[\"test_file\"])\n",
    "\n",
    "\n",
    "    max_seq_length= int(config[\"max_seq_length\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    try: \n",
    "        early_stop = config[\"early_stop\"]\n",
    "    except:\n",
    "        early_stop = 5\n",
    "\n",
    "    try:\n",
    "        save_model = config[\"save_model\"]\n",
    "    except: \n",
    "        save_model = False\n",
    "\n",
    "\n",
    "    learning_rate = float(config[\"lr\"]) \n",
    "    model_path = config['pretrained_model_path']\n",
    "    num_epochs = config['epochs']\n",
    "\n",
    "    #---------------------------------------------------------\n",
    "    print(\"[INFO] step (2) check checkpoit directory and report file:\")\n",
    "    ckpt_dir = config[\"ckpt_dir\"] + \"/\"\n",
    "\n",
    "    #-------------------------------------------------------\n",
    "    print(\"[INFO] step (3) load label to number dictionary:\")\n",
    "    \n",
    "    delimiter = None\n",
    "    if(str(train_file).endswith('tsv')):\n",
    "        delimiter = '\\t'\n",
    "    df = pd.read_csv(train_file, delimiter=delimiter)\n",
    "    labels = df[label_col].tolist()\n",
    "    is_regression = False\n",
    "    if(isinstance(labels[0], float)):\n",
    "        is_regression = True\n",
    "        lab2ind = {'float':0}\n",
    "    \n",
    "    unique_labels = list(set(labels))\n",
    "    lab2ind = {l:ind for ind,l in enumerate(unique_labels)}\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        num_workers = config['num_workers']\n",
    "    except:\n",
    "        num_workers = 1\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] train_file\", train_file)\n",
    "    print(\"[INFO] dev_file\", dev_file)\n",
    "    print(\"[INFO] test_file\", test_file)\n",
    "    print(\"[INFO] num_epochs\", num_epochs)\n",
    "    print(\"[INFO] model_path\", model_path)\n",
    "    print(\"[INFO] max_seq_length\", max_seq_length)\n",
    "    print(\"[INFO] batch_size\", batch_size)\n",
    "    print(\"[INFO] Number of Classes\", len(lab2ind))\n",
    "    print(\"[INFO] Number of Workers\", num_workers)\n",
    "    print(\"[INFO] step (4) Use defined funtion to extract tokanize data\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if(is_regression):\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    print(\"[INFO] step (5) Create an iterator of data with torch DataLoader.\")\n",
    "\n",
    "    train_dataloader = regular_encode(train_file, tokenizer, lab2ind, True, batch_size=batch_size, maxlen = max_seq_length, mode = \"train\",\n",
    "                                     text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    validation_dataloader = regular_encode(dev_file, tokenizer, lab2ind, True, batch_size=batch_size, maxlen = max_seq_length, mode = \"evaluate\",\n",
    "                                     text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "\n",
    "    print(\"[INFO] step (6) run with parallel CPU/GPUs\")\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() == 1:\n",
    "            print(\"Run\",model_path, \"with one GPU\")\n",
    "            model = model.to(device)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------\n",
    "    print(\"[INFO] step (7) set Parameters, schedules, and loss function:\")\n",
    "    global max_grad_norm\n",
    "    max_grad_norm = 1.0\n",
    "    try:\n",
    "        warmup_proportion = config[\"warmup_proportion\"]\n",
    "    except: \n",
    "        warmup_proportion = 0.06\n",
    "\n",
    "    num_training_steps\t= len(train_dataloader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps * warmup_proportion\n",
    "    ### In Transformers, optimizer and schedules are instantiated like this:\n",
    "    # Note: AdamW is a class from the huggingface library\n",
    "    # the 'W' stands for 'Weight Decay\"\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # schedules\n",
    "    total_params = list(model.named_parameters())\n",
    "\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(total_params, num_training_steps, num_warmup_steps, weight_decay, learning_rate, is_constant_lr)\n",
    "\n",
    "    metric = accuracy\n",
    "    \n",
    "    print(\"[INFO] step (8) start fine_tuning\")\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] step (8) start fine_tuning\")\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, isPair = isPair)\t \n",
    "        eval_loss, eval_result = evaluate(model, validation_dataloader, metric, isPair=isPair)\n",
    "        print(f'Train Loss: {train_loss}')\n",
    "        print(eval_result)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334840ab-6d59-442a-9512-a48c77559479",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58f6ea0-b179-4fb7-bfe1-cb03ccac0b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tawkat/ENV_1/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] step (2) check checkpoit directory and report file:\n",
      "[INFO] step (3) load label to number dictionary:\n",
      "[INFO] train_file ./sst2_tiny.csv\n",
      "[INFO] dev_file ./sst2_tiny.csv\n",
      "[INFO] test_file ./sst2_tiny.csv\n",
      "[INFO] num_epochs 3\n",
      "[INFO] model_path allenai/longformer-base-4096\n",
      "[INFO] max_seq_length 4096\n",
      "[INFO] batch_size 5\n",
      "[INFO] Number of Classes 2\n",
      "[INFO] Number of Workers 1\n",
      "[INFO] step (4) Use defined funtion to extract tokanize data\n",
      "[INFO] step (5) Create an iterator of data with torch DataLoader.\n",
      "./sst2_tiny.csv Dataset: (10, 3)\n",
      "./sst2_tiny.csv Dataset: (10, 3)\n",
      "[INFO] step (6) run with parallel CPU/GPUs\n",
      "[INFO] step (7) set Parameters, schedules, and loss function:\n",
      "[INFO] step (8) start fine_tuning\n",
      "[INFO] step (8) start fine_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 1/2 [00:58<00:58, 58.70s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 2/2 [01:56<00:00, 58.42s/it]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [03:41<07:23, 221.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0\n",
      "{'accuracy': 0.6}\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 1/2 [00:59<00:59, 59.10s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 2/2 [01:56<00:00, 58.43s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [07:22<03:41, 221.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0\n",
      "{'accuracy': 0.6}\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  50%|█████     | 1/2 [00:59<00:59, 59.50s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 2/2 [01:57<00:00, 58.63s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [11:04<00:00, 221.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0\n",
      "{'accuracy': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['task_name'] = \"Binary Sentiment Classification\"\n",
    "config['text_column_1'] = \"sentence\"\n",
    "config['text_column_2'] = None\n",
    "config['label_column'] = \"label\"\n",
    "config['data_dir'] = \"./\"\n",
    "config['train_file'] = \"sst2_tiny.csv\"\n",
    "config['dev_file'] = \"sst2_tiny.csv\"\n",
    "config['test_file'] = \"sst2_tiny.csv\"\n",
    "config['is_constant_lr'] = False\n",
    "config['max_seq_length'] = 4096\n",
    "config['batch_size'] = 5\n",
    "config['early_stop'] = 5\n",
    "config['save_model'] = False\n",
    "config['lr'] = 0.00005\n",
    "config['pretrained_model_path'] = \"allenai/longformer-base-4096\"\n",
    "config['epochs'] = 3\n",
    "config['ckpt_dir'] = \"ckpt\"\n",
    "config['warmup_proportion'] = 0.05\n",
    "config['metric'] = \"accuracy\"\n",
    "config['num_workers'] = 1\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "fine_tuning(model, tokenizer, config, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993037c-0c24-4e80-9799-eac9bd022c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
