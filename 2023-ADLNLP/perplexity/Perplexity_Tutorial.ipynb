{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki559_s5ap7z"
      },
      "source": [
        "# **Perplexity of fixed-length models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eppoA_piap71"
      },
      "source": [
        "## **Definition:**\n",
        "\n",
        "Perplexity (PPL) is a widely used metric that estimates how well an autoregressive language model predicts a text in a given context. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. \n",
        "## **Formula:**\n",
        "\n",
        "If we have a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$, then the perplexity of $X$ is:\n",
        "\n",
        "$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\},$$\n",
        "\n",
        "where $\\log p_\\theta (x_i|x_{<i})$ is the log-likelihood of the $i^{th}$ token conditioned on the prior tokens $x_{<i}$ according to our model. \n",
        "\n",
        "\n",
        "#####Note:\n",
        "\n",
        "The tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Ofxz8Bap73"
      },
      "source": [
        "## **Calculating PPL with fixed-length models**\n",
        "\n",
        "# Unlimited Context Size\n",
        "\n",
        "If we weren't limited by a model's context size, we would evaluate the model's perplexity by autoregressively\n",
        "factorizing a sequence and conditioning on the **entire processed subsequence** at each step, as shown below.\n",
        "\n",
        "<img width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl.gif\n",
        "\"/>\n",
        "\n",
        "\n",
        "# Fixed Context Size\n",
        "\n",
        "When working with autoregressive language model, we typically have a constraint on the number of tokens the model can process. [GPT-2](hhttps://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe), for example, has a fixed length of 1,024 tokens, so we cannot calculate $p_\\theta(x_t|x_{<t})$ directly when $t$ is greater than 1,024.\n",
        "\n",
        "Instead, the sequence is typically **broken into** subsequences equal to the model's maximum input size . If a model's max input size is $k$ (e.g., $k$=1,024 for GPT2 and $k$= 2,048 for GPT3), we then approximate the likelihood of a token $x_t$ by conditioning only on the $k-1$ tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\n",
        "sequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed\n",
        "log-likelihoods of each segment independently. The example below shows the previous example with a fixed context size of $k=5$.\n",
        "\n",
        "<img width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl2.gif\"/>\n",
        "\n",
        "\n",
        "# Sliding-Window Strategy\n",
        "\n",
        "This is quick to compute since the perplexity of each segment can be computed in one forward pass, but serves as a poor\n",
        "approximation of the fully-factorized perplexity and will typically yield a higher (worse) PPL because the model will\n",
        "have less context at most of the prediction steps.\n",
        "\n",
        "Instead, the PPL of fixed-length models should be evaluated with a **sliding-window strategy**. This involves repeatedly\n",
        "sliding the context window so that the model has more context when making each prediction. The example below  uses sliding-window size of $k=5$.\n",
        "\n",
        "<img width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\" src=\"https://raw.githubusercontent.com/Nagoudi/Perplexity/main/ppl3.gif\"/>\n",
        "\n",
        "This is a closer approximation to the true decomposition of the sequence probability and will typically yield a more\n",
        "favorable score. The downside is that it requires a separate forward pass for each token in the corpus. A good\n",
        "practical compromise is to employ a **strided sliding window**, moving the context by larger strides rather than sliding by\n",
        "1 token a time. This allows computation to proceed much faster while still giving the model a large context to make\n",
        "predictions at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgJJledfap74"
      },
      "source": [
        "# Example: Calculating perplexity with GPT-2 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's first install Transformers from HuggingFace 🤗"
      ],
      "metadata": {
        "id": "IWrjWl2RwGlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers and datasets installation\n",
        "! pip install transformers datasets"
      ],
      "metadata": {
        "id": "IGPw_hBxuPKn",
        "outputId": "df0451a8-1794-4bc2-e173-6807de19c78a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, xxhash, urllib3, multiprocess, responses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the GPT-2's model and Toknizer"
      ],
      "metadata": {
        "id": "oH1XLv6jNWVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVfvXnhCap76"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "## download GPT model\n",
        "device = \"cuda\"\n",
        "model_id = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and toknize WikiText-2 dataset "
      ],
      "metadata": {
        "id": "COs3ywloNeka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOoZEG_wap77",
        "outputId": "4abef20f-cfad-4d84-a282-77bc86f8c317",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "print(test[\"text\"][3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\") # Since this dataset is small and we're just doing one forward pass over the set, we can just load and encode the entire dataset in memory."
      ],
      "metadata": {
        "id": "e2c6Uwj3B2ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the perplexity using the sliding-window strategie\n",
        "\n",
        "With 🤗 Transformers, we can simply:\n",
        " \n",
        "\n",
        "1.   Pass the `input_ids` as the `labels` to our model\n",
        "2.   Average negative log-likelihood for each token is returned as the loss.\n",
        "\n",
        "\n",
        "#### **Note 1:**\n",
        "\n",
        "With our sliding window approach, however, there is overlap in the tokens we pass to the model at each iteration. We don't want the log-likelihood for the tokens we're just treating as context to be included in our loss, so we can set these targets to `-100` so that they are ignored. \n",
        "\n",
        "#### **Calculating perplexity:** \n",
        "\n",
        "The following is an example of how we could do this with a stride of `512`. This means that the model will have at least 512 tokens\n",
        "for context when calculating the conditional likelihood of any one token (provided there are 512 preceding tokens\n",
        "available to condition on).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WS-B2AT6OXVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-PvsBEap78",
        "outputId": "12e87274-692e-44dd-ed9a-a8bbf2db1bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/562 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 1/562 [00:02<22:56,  2.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          | 2/562 [00:02<10:06,  1.08s/it]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 4/562 [00:02<04:18,  2.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|          | 6/562 [00:02<02:41,  3.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "  1%|▏         | 8/562 [00:03<01:57,  4.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 10/562 [00:03<01:33,  5.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 12/562 [00:03<01:20,  6.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 14/562 [00:03<01:11,  7.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 16/562 [00:03<01:05,  8.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 18/562 [00:04<01:00,  8.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▎         | 20/562 [00:04<00:58,  9.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 22/562 [00:04<00:56,  9.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 24/562 [00:04<00:55,  9.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▍         | 26/562 [00:04<00:54,  9.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▍         | 28/562 [00:05<00:53, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▌         | 30/562 [00:05<00:52, 10.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|▌         | 32/562 [00:05<00:52, 10.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|▌         | 34/562 [00:05<00:52, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "  6%|▋         | 36/562 [00:05<00:51, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 38/562 [00:06<00:51, 10.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 40/562 [00:06<00:51, 10.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 42/562 [00:06<00:50, 10.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 44/562 [00:06<00:50, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 46/562 [00:06<00:50, 10.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▊         | 48/562 [00:07<00:49, 10.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▉         | 50/562 [00:07<00:49, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▉         | 52/562 [00:07<00:49, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|▉         | 54/562 [00:07<00:49, 10.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|▉         | 56/562 [00:07<00:49, 10.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 58/562 [00:08<00:49, 10.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█         | 60/562 [00:08<00:48, 10.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█         | 62/562 [00:08<00:48, 10.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█▏        | 64/562 [00:08<00:48, 10.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 66/562 [00:08<00:48, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 68/562 [00:08<00:48, 10.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 70/562 [00:09<00:48, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 72/562 [00:09<00:48, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 74/562 [00:09<00:47, 10.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▎        | 76/562 [00:09<00:47, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▍        | 78/562 [00:09<00:47, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▍        | 80/562 [00:10<00:47, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▍        | 82/562 [00:10<00:47, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▍        | 84/562 [00:10<00:46, 10.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 15%|█▌        | 86/562 [00:10<00:46, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 88/562 [00:10<00:46, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 90/562 [00:11<00:45, 10.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▋        | 92/562 [00:11<00:45, 10.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 94/562 [00:11<00:45, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 96/562 [00:11<00:45, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 17%|█▋        | 98/562 [00:11<00:45, 10.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 100/562 [00:12<00:45, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 102/562 [00:12<00:44, 10.25it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▊        | 104/562 [00:12<00:44, 10.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 106/562 [00:12<00:44, 10.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 108/562 [00:12<00:44, 10.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|█▉        | 110/562 [00:13<00:44, 10.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|█▉        | 112/562 [00:13<00:44, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 114/562 [00:13<00:44, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 116/562 [00:13<00:43, 10.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 118/562 [00:13<00:43, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██▏       | 120/562 [00:14<00:43, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 122/562 [00:14<00:43, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 124/562 [00:14<00:43, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 126/562 [00:14<00:42, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 128/562 [00:14<00:42, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 130/562 [00:15<00:42, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 132/562 [00:15<00:42, 10.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 134/562 [00:15<00:42, 10.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 136/562 [00:15<00:41, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▍       | 138/562 [00:15<00:41, 10.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▍       | 140/562 [00:16<00:41, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▌       | 142/562 [00:16<00:41, 10.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▌       | 144/562 [00:16<00:41, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▌       | 146/562 [00:16<00:40, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 26%|██▋       | 148/562 [00:16<00:40, 10.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 150/562 [00:17<00:40, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 152/562 [00:17<00:40, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 154/562 [00:17<00:40, 10.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 156/562 [00:17<00:39, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 158/562 [00:17<00:39, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 160/562 [00:18<00:39, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▉       | 162/562 [00:18<00:39, 10.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▉       | 164/562 [00:18<00:39, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 166/562 [00:18<00:38, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 168/562 [00:18<00:38, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 170/562 [00:19<00:38, 10.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███       | 172/562 [00:19<00:38, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███       | 174/562 [00:19<00:38, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 31%|███▏      | 176/562 [00:19<00:38, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 178/562 [00:19<00:37, 10.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 180/562 [00:19<00:37, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 182/562 [00:20<00:37, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 184/562 [00:20<00:37, 10.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 186/562 [00:20<00:37, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 188/562 [00:20<00:36, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|███▍      | 190/562 [00:20<00:36, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 34%|███▍      | 192/562 [00:21<00:36, 10.18it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 194/562 [00:21<00:36, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 196/562 [00:21<00:36, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▌      | 198/562 [00:21<00:35, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 200/562 [00:21<00:35, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 202/562 [00:22<00:35, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▋      | 204/562 [00:22<00:35, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 206/562 [00:22<00:35, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 208/562 [00:22<00:35, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 210/562 [00:22<00:34, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 212/562 [00:23<00:34, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 214/562 [00:23<00:34, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 216/562 [00:23<00:34, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 218/562 [00:23<00:34, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 220/562 [00:23<00:33, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|███▉      | 222/562 [00:24<00:33, 10.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|███▉      | 224/562 [00:24<00:33, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 226/562 [00:24<00:33, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 228/562 [00:24<00:33, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 230/562 [00:24<00:32, 10.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████▏     | 232/562 [00:25<00:32, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 234/562 [00:25<00:32, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 236/562 [00:25<00:32, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 42%|████▏     | 238/562 [00:25<00:32, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 240/562 [00:25<00:31, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 242/562 [00:26<00:31, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 244/562 [00:26<00:31, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 246/562 [00:26<00:31,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 248/562 [00:26<00:31, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 250/562 [00:26<00:30, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▍     | 252/562 [00:27<00:30, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 45%|████▌     | 254/562 [00:27<00:30, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 256/562 [00:27<00:30, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 258/562 [00:27<00:30, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▋     | 260/562 [00:27<00:29, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 262/562 [00:28<00:29, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 264/562 [00:28<00:29, 10.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 47%|████▋     | 266/562 [00:28<00:29, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 268/562 [00:28<00:29, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 270/562 [00:28<00:28, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 272/562 [00:29<00:28, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▉     | 274/562 [00:29<00:28, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▉     | 276/562 [00:29<00:28, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▉     | 278/562 [00:29<00:28, 10.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|████▉     | 280/562 [00:29<00:27, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 282/562 [00:30<00:27, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 284/562 [00:30<00:27, 10.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 286/562 [00:30<00:27, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 288/562 [00:30<00:27, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 290/562 [00:30<00:27, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 292/562 [00:31<00:26, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 294/562 [00:31<00:26, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 296/562 [00:31<00:26, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 298/562 [00:31<00:26, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 300/562 [00:31<00:26, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▎    | 302/562 [00:32<00:25, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 304/562 [00:32<00:25, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 306/562 [00:32<00:25, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▍    | 308/562 [00:32<00:25, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▌    | 310/562 [00:32<00:25, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 312/562 [00:33<00:24, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 314/562 [00:33<00:24, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 56%|█████▌    | 316/562 [00:33<00:24, 10.12it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 318/562 [00:33<00:24, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 320/562 [00:33<00:24, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 322/562 [00:34<00:23, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 324/562 [00:34<00:23, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 326/562 [00:34<00:23, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 328/562 [00:34<00:23, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▊    | 330/562 [00:34<00:23, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 332/562 [00:35<00:22, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 334/562 [00:35<00:22, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 336/562 [00:35<00:22, 10.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 338/562 [00:35<00:22, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 340/562 [00:35<00:22, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████    | 342/562 [00:36<00:21, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 61%|██████    | 344/562 [00:36<00:21, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 346/562 [00:36<00:21, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 348/562 [00:36<00:21, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 350/562 [00:36<00:21, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 352/562 [00:37<00:20, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 354/562 [00:37<00:20, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 63%|██████▎   | 356/562 [00:37<00:20, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▎   | 358/562 [00:37<00:20, 10.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▍   | 360/562 [00:37<00:20, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 64%|██████▍   | 362/562 [00:38<00:19, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 364/562 [00:38<00:19, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▌   | 366/562 [00:38<00:19, 10.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▌   | 368/562 [00:38<00:19, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66%|██████▌   | 370/562 [00:38<00:19, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66%|██████▌   | 372/562 [00:39<00:18, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 374/562 [00:39<00:18, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 376/562 [00:39<00:18, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 378/562 [00:39<00:18, 10.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 380/562 [00:39<00:18, 10.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 382/562 [00:40<00:17, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 384/562 [00:40<00:17, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▊   | 386/562 [00:40<00:17,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▉   | 388/562 [00:40<00:17, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▉   | 390/562 [00:40<00:17,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|██████▉   | 391/562 [00:41<00:20,  8.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|██████▉   | 392/562 [00:41<00:21,  7.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|██████▉   | 393/562 [00:41<00:20,  8.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 395/562 [00:41<00:18,  8.84it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████   | 397/562 [00:41<00:17,  9.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████   | 399/562 [00:41<00:18,  8.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████   | 400/562 [00:42<00:18,  8.79it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████▏  | 401/562 [00:42<00:20,  8.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 402/562 [00:42<00:19,  8.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 404/562 [00:42<00:17,  8.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 72%|███████▏  | 406/562 [00:42<00:16,  9.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 408/562 [00:42<00:16,  9.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 409/562 [00:43<00:17,  8.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 410/562 [00:43<00:18,  8.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 411/562 [00:43<00:18,  7.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 413/562 [00:43<00:16,  8.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 415/562 [00:43<00:15,  9.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 417/562 [00:43<00:15,  9.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 418/562 [00:44<00:15,  9.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 419/562 [00:44<00:14,  9.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 420/562 [00:44<00:14,  9.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 422/562 [00:44<00:14,  9.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 424/562 [00:44<00:13,  9.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 426/562 [00:44<00:13,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 428/562 [00:45<00:13,  9.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▋  | 429/562 [00:45<00:13,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 431/562 [00:45<00:13, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 433/562 [00:45<00:12, 10.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 77%|███████▋  | 435/562 [00:45<00:12, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 437/562 [00:45<00:12, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 439/562 [00:46<00:12,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 440/562 [00:46<00:12,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 441/562 [00:46<00:12,  9.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 443/562 [00:46<00:11,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 79%|███████▉  | 445/562 [00:46<00:11, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 447/562 [00:46<00:11, 10.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|███████▉  | 449/562 [00:47<00:11, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 451/562 [00:47<00:11,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 452/562 [00:47<00:11,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 453/562 [00:47<00:11,  9.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 455/562 [00:47<00:10,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 456/562 [00:47<00:10,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████▏ | 457/562 [00:47<00:10,  9.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████▏ | 458/562 [00:48<00:10,  9.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 459/562 [00:48<00:10,  9.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 460/562 [00:48<00:10,  9.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 461/562 [00:48<00:10,  9.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 462/562 [00:48<00:10,  9.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 463/562 [00:48<00:09,  9.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 465/562 [00:48<00:09, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 467/562 [00:48<00:09, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 469/562 [00:49<00:09, 10.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 471/562 [00:49<00:09, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 473/562 [00:49<00:08,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 474/562 [00:49<00:08,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 475/562 [00:49<00:08,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 476/562 [00:49<00:08,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▍ | 477/562 [00:49<00:08,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 478/562 [00:50<00:08,  9.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 479/562 [00:50<00:08,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 481/562 [00:50<00:08, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▌ | 483/562 [00:50<00:07, 10.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 485/562 [00:50<00:07, 10.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 487/562 [00:50<00:07, 10.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 489/562 [00:51<00:07, 10.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 87%|████████▋ | 491/562 [00:51<00:07, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 493/562 [00:51<00:06, 10.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 495/562 [00:51<00:06, 10.05it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 497/562 [00:51<00:06,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▊ | 498/562 [00:52<00:06,  9.98it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 499/562 [00:52<00:06,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 500/562 [00:52<00:06,  9.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 501/562 [00:52<00:06,  9.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 503/562 [00:52<00:05,  9.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|████████▉ | 504/562 [00:52<00:05,  9.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 506/562 [00:52<00:05,  9.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 507/562 [00:52<00:05,  9.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 508/562 [00:53<00:05,  9.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 509/562 [00:53<00:05,  9.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 510/562 [00:53<00:05,  9.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 511/562 [00:53<00:05,  9.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 512/562 [00:53<00:05,  9.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 513/562 [00:53<00:04,  9.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████▏| 514/562 [00:53<00:04,  9.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 516/562 [00:53<00:04,  9.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 518/562 [00:54<00:04, 10.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 520/562 [00:54<00:04, 10.02it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 522/562 [00:54<00:03, 10.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 523/562 [00:54<00:03, 10.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 524/562 [00:54<00:03,  9.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 93%|█████████▎| 525/562 [00:54<00:03,  9.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▎| 526/562 [00:54<00:03,  9.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 527/562 [00:54<00:03,  9.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 529/562 [00:55<00:03,  9.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 530/562 [00:55<00:03,  9.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 94%|█████████▍| 531/562 [00:55<00:03,  8.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 533/562 [00:55<00:03,  9.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 534/562 [00:55<00:03,  9.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▌| 536/562 [00:55<00:02,  9.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 537/562 [00:56<00:02,  9.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 538/562 [00:56<00:02,  9.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 539/562 [00:56<00:02,  9.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▌| 540/562 [00:56<00:02,  9.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 96%|█████████▋| 541/562 [00:56<00:02,  9.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 543/562 [00:56<00:02,  9.28it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 544/562 [00:56<00:01,  9.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 546/562 [00:57<00:01,  9.48it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 547/562 [00:57<00:01,  9.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 549/562 [00:57<00:01,  9.70it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 550/562 [00:57<00:01,  9.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 551/562 [00:57<00:01,  9.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 552/562 [00:57<00:01,  9.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 98%|█████████▊| 553/562 [00:57<00:00,  9.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▊| 554/562 [00:57<00:00,  9.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 555/562 [00:57<00:00,  9.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 556/562 [00:58<00:00,  9.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 99%|█████████▉| 558/562 [00:58<00:00,  9.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|█████████▉| 560/562 [00:58<00:00,  9.57it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions # For GPT-2 the max_length is 1024\n",
        "stride = 512  # we will use 512 tokens as sliding-window size \n",
        "seq_len = encodings.input_ids.size(1) # seq_len of the WikiText-2. \n",
        "\n",
        "nlls = []\n",
        "prev_end_loc = 0\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        # loss is calculated using CrossEntropyLoss which averages over input tokens.\n",
        "        # Multiply it with trg_len to get the summation instead of average.\n",
        "        # We will take average over all the tokens to get the true average\n",
        "        # in the last step of this example.\n",
        "        neg_log_likelihood = outputs.loss * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The perplexity of GPT-2 on WikiText-2 is:', ppl.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0HHItdFYadj",
        "outputId": "210291fd-d3ac-4340-8918-9b4aa1a92fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity of GPT-2 on WikiText-2 is: 25.170446395874023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source\n",
        "\n",
        "[1]: [GPT-2 Paper](https://www.semanticscholar.org/paper-Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "\n",
        "[2]  https://huggingface.co/docs/transformers\n",
        "\n",
        "[3]  https://huggingface.co/docs/transformers/perplexity."
      ],
      "metadata": {
        "id": "2xN3lIa40QTl"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}