{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SP model.\n",
    "For larger corpora you will need to set a larger vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAG0mLgAleFR"
   },
   "outputs": [],
   "source": [
    "#@title Train a SP model. \n",
    "#@markdown For larger corpora you will need to set a larger vocab size.\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train(input='train.en', model_prefix='en', vocab_size=8000)  \n",
    "spm.SentencePieceTrainer.train(input='train.fr', model_prefix='fr', vocab_size=9000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNXO7jhWmgy4"
   },
   "source": [
    "Providing the *.model and *.vocab to OpenNMT config yaml file will allow you to train with it:\n",
    "\n",
    "\n",
    "    src_subword_model: fr.model\n",
    "    tgt_subword_model: en.model\n",
    "    src_subword_vocab: fr.vocab\n",
    "    tgt_subword_vocab: en.vocab\n",
    "\n",
    "adding sentencepiece to the corpora as needed:\n",
    "\n",
    "    data:\n",
    "       corpus_1:\n",
    "          path_src: train.fr\n",
    "          path_tgt: train.en\n",
    "          transforms: [sentencepiece, filtertoolong]\n",
    "      valid:\n",
    "          path_src: val.fr\n",
    "          path_tgt: val.en\n",
    "          transforms: [sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting SPM\n",
    "\n",
    "It's possible to convert back and forth from sentence piece (in fact OpenNMT will give the outputs with this tokenization so you'll need ot detokenize before calculating BLEU). For use with regular pytorch you can just apply the already trained model as such (examples from Danish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4k1-ZlzMPA6",
    "outputId": "6d2ff7fa-04d3-404b-b339-4f4fe883f07b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: På Det Blandede EØS-Udvalgs vegne\n",
      "\n",
      "SentencePiece Tokens: ['▁På', '▁Det', '▁Blande', 'de', '▁EØS', '-', 'Udvalg', 's', '▁vegne']\n",
      "Expected OpenNMT output: ▁På ▁Det ▁Blande de ▁EØS - Udvalg s ▁vegne\n",
      "List of re-split tokens['▁På', '▁Det', '▁Blande', 'de', '▁EØS', '-', 'Udvalg', 's', '▁vegne']\n",
      "Detokenized: På Det Blandede EØS-Udvalgs vegne\n"
     ]
    }
   ],
   "source": [
    "#@title Converting SPM\n",
    "#@markdown It's possible to convert back and forth from sentence piece (in fact OpenNMT will give the outputs with this tokenization so you'll need ot detokenize before calculating BLEU). For use with regular pytorch you can \n",
    "#@markdown just apply the already trained model as such (examples from Danish)\n",
    "\n",
    "#Loading and converting to sentence piece format\n",
    "with open(\"train.da\",'r',encoding='utf-8') as fp:\n",
    "  sent = fp.readline()\n",
    "  fp.close()\n",
    "sp = spm.SentencePieceProcessor(model_file='da.model')\n",
    "print(\"Input Sentence: \"+sent)\n",
    "tokd = sp.encode(sent,out_type=str)\n",
    "print(\"SentencePiece Tokens: \"+str(tokd))\n",
    "output = \" \".join(tokd)\n",
    "\n",
    "# How to convert from SentencePiece format to \"normal\" text\n",
    "# START HERE IF USING OpenNMT\n",
    "print(\"Expected OpenNMT output: \"+output)\n",
    "split_out = output.split(\" \")\n",
    "print(\"List of re-split tokens\"+str(split_out))\n",
    "detokd = sp.decode(split_out).replace(\"▁\", \" \") #capture any remaining underscores that escape\n",
    "print(\"Detokenized: \"+detokd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU\n",
    "\n",
    "The updated BLEU script works for detokenized text, so we if we've saved the outputs of the detokenization from above to \"predictions.detokd\" we can use BLEU as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOVcO20BonZC"
   },
   "outputs": [],
   "source": [
    "#@title BLEU\n",
    "#@markdown The updated BLEU script works for detokenized text, so we if we've saved the outputs of the detokenization from above to \"predictions.detokd\" we can use BLEU as follows:\n",
    "\n",
    "!perl  OpenNMT-py/tools/multi-bleu-detok.perl gold_standard < predictions.detokd\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SentencePieceTok.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
