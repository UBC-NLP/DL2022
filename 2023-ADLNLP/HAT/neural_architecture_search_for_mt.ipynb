{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21luw3P3HdAN"
      },
      "source": [
        "# Neural Architecture Search for Efficient Machine Translation Models\n",
        "\n",
        "In this tutorial, we will go over the high-level theory and implementation details of the **neural architecture search (NAS)** pipeline for identifying efficient machine translation models. The machine translations models are based on the classical encoder-decoder Transformer architectures. These models are trained on machine translation benchmarks from scratch (no pretraining) to convergence. This tutorial borrows the theory and implementation from [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf), which is the state-of-the-art NAS framework to build efficient autoregressive machine translation models.\n",
        "\n",
        "This notebook was created by Ganesh Jawahar (ganeshjwhr@gmail.com).\n",
        "\n",
        "## Prerequisites\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "- [Transformers](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "## Problem Setting (General)\n",
        "The goal of neural architecture search is to identify architectures that maximize the **accuracy for a user-defined task** as much as possible, while satisfying user-defined **hardware constraints**. Specifically, the input to the neural architecture search is:\n",
        "- **Task:** The NLP task (e.g., autocomplete, machine translation) that the Transformer model should solve.\n",
        "- **Search Space:** Set of candidate Transformer architectures (e.g., varying number of layers, attention heads) that can solve the **task**.\n",
        "- **Constraint:** Constraint on the footprint metric (e.g., $\\leq16$ MB memory or $\\leq200$ ms latency) that the architecture must satisfy.\n",
        "- **Accuracy:** The metric used to quantify the accuracy of the model on the **task**.\n",
        "\n",
        "The method should output the architecture that maximizes the **accuracy** of the model on the **task** from the **search space**, while satisfying the **constraint**. \n",
        "\n",
        "## Problem Setting (This Tutorial)\n",
        "In this tutorial, \n",
        "- **Task:** Machine Translation Task (e.g., WMT 2014 English to German)\n",
        "- **Search Space:**  Set of candidate encoder-decoder Transformer architectures with varying number of decoder layers, embedding size, attention heads (self-attention and cross-attention), feed-forward network (FFN) intermediate size and arbitrary encoder-decoder attention. For arbitrary encoder-decoder attention, -1 means attending to last one encoder layer, 1 means last two encoder layers, 2 means last three encoder layers.\n",
        "\n",
        "| Attributes | Dimensions |\n",
        "| --- | --- |\n",
        "| Encoder-Embedding-Size | [640, 512] |\n",
        "| Decoder-Embedding-Size | [640, 512] |\n",
        "| \\#Encoder-Layers | [6] |\n",
        "| \\#Decoder-Layers | [1, 2, 3, 4, 5, 6] |\n",
        "| Encoder-QKV-Dim | 512 |\n",
        "| Decoder-QKV-Dim | 512 |\n",
        "| \\#Encoder-Self-Att-Heads (Per Layer) | [4, 8] |\n",
        "| \\#Decoder-Self-Att-Heads (Per Layer) | [4, 8] |\n",
        "| \\#Decoder-Cross-Att-Heads (Per Layer) | [4, 8] |\n",
        "| \\#Decoder-Arbitrary-Att (Per Layer) | [-1, 1, 2] |\n",
        "| Encoder-FFN-Intermediate-Size (Per Layer) | [1024, 2048, 3072] |\n",
        "| Decoder-FFN-Intermediate-Size (Per Layer) | [1024, 2048, 3072] |\n",
        "\n",
        "- **Constraint:** $\\leq200$ milliseconds latency (time taken by the model to encode the source sentence and generate the translation sentence in a target hardware (Colab GPU in this example))\n",
        "- **Accuracy:** BLEU score\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0opSWSHDUYrn"
      },
      "source": [
        "## Hardware-aware Transformers (Solution)\n",
        "\n",
        "Hardware-aware Transformers (HAT) is a popular NAS framework to solve the problem. HAT has the following stages in the pipeline:\n",
        "1. **Superet training** - Train a performance estimator that can quickly provide the accuracy of an architecture from the search space\n",
        "2. **Collect hardware latency datasets** - Generate a latency dataset with sample architectures and their corresponding latency measured on target hardware \n",
        "3. **Train latency predictor** - Train a latency estimator on the generated latency dataset\n",
        "4. **Evolutionary search** - Identifies the efficient architecture with accuracy and latency of a candidate architecture from the performance estimator and latency estimator respectively.\n",
        "5. **Train efficient architecture from scratch** - Trains the efficient architecture from scratch to convergence.\n",
        "\n",
        "![HAT block diagram](https://drive.google.com/uc?export=view&id=1ysb21_UiSqahCm_dtrY8aoIZVKPnL6eE)\n",
        "\n",
        "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
        "\n",
        "## HAT generated sample architectures\n",
        "Some sample architectures generated by HAT when target hardware is Raspberry Pi (left side) and Titan XP (right side):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1plfV3ZqxaqWznF8oJ5XLAYtHAXos2-Fy\" alt=\"HAT generated architectures\" width=\"700\"/>\n",
        "\n",
        "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgpEckM3hxAZ"
      },
      "source": [
        "## HAT Implementation\n",
        "\n",
        "### 0.1 Installation\n",
        "\n",
        "Install HAT by running the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtGhECPBwLrx",
        "outputId": "6f821083-12bb-48e1-9292-31855d1d0fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hardware-aware-transformers'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 282 (delta 32), reused 23 (delta 23), pack-reused 193\u001b[K\n",
            "Receiving objects: 100% (282/282), 17.09 MiB | 30.38 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n",
            "/content/hardware-aware-transformers\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/hardware-aware-transformers\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (1.15.1)\n",
            "Collecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (2022.6.2)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (4.64.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq==0.8.0) (0.29.33)\n",
            "Collecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting ujson\n",
            "  Downloading ujson-5.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardx\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq==0.8.0) (2.21)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu->fairseq==0.8.0) (4.9.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu->fairseq==0.8.0) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->fairseq==0.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->fairseq==0.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->fairseq==0.8.0) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardx->fairseq==0.8.0) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardx->fairseq==0.8.0) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->fairseq==0.8.0) (4.4.0)\n",
            "Building wheels for collected packages: fastBPE, sacremoses\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp38-cp38-linux_x86_64.whl size=759289 sha256=153ea40d430b328057397adb0be2960acd41f11092fc63f79216faa139ab6f10\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a8/c6/07fb6443539116f05252337e751bbccd2bd2dd6c585ba94ca8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=01d3458961f8bb6070781a40d5b0a2e13446a18edbc4d7337c35620001dd0e39\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built fastBPE sacremoses\n",
            "Installing collected packages: fastBPE, ujson, tensorboardx, sacremoses, portalocker, configargparse, colorama, sacrebleu, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed colorama-0.4.6 configargparse-1.5.3 fairseq-0.8.0 fastBPE-0.1.0 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 tensorboardx-2.6 ujson-5.7.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mit-han-lab/hardware-aware-transformers.git\n",
        "%cd hardware-aware-transformers\n",
        "!pip install --editable ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.get_device_name(0)\n",
        "    print(f'GPU type: {device}')\n",
        "else:\n",
        "    print('No GPU available')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AugXrd8kj-3B",
        "outputId": "695fb2ec-4749-4b5a-da70-d41d764d412f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU type: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Y27shIftkF"
      },
      "source": [
        "### 0.2 Download data\n",
        "\n",
        "Download the preprocessed data for the machine translation task. The syntax is:\n",
        "\n",
        "`bash configs/[task_name]/get_preprocessed.sh`\n",
        "- where `[task_name]` can be `wmt14.en-de`, `wmt14.en-fr`, `wmt19.en-de` and `iwslt14.de-en`.\n",
        "\n",
        "In this tutorial, we will focus on WMT 2014 English to German (`wmt14.en-de`). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNuC3wU0HcLH",
        "outputId": "11b613e8-de8e-4660-ab4e-864c2eeaa8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-13 10:51:14--  https://www.dropbox.com/s/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz [following]\n",
            "--2023-02-13 10:51:15--  https://www.dropbox.com/s/raw/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com/cd/0/inline/B2ZEth56qq7FygA8sEdu5RbjyGq2MDgrPVKyYatMvGupAZ7sft2vpKHy1py4__RXoK3Vn-YnS28H1wc40XxcpWCzj2da9aXN-JozbkrBZ_ywFRemrHjsQOwydgA5rWi2Mw1i7qHiqXz2Vug76fEiaE9zvitkBKWlxURM5d3jcIrHvw/file# [following]\n",
            "--2023-02-13 10:51:15--  https://uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com/cd/0/inline/B2ZEth56qq7FygA8sEdu5RbjyGq2MDgrPVKyYatMvGupAZ7sft2vpKHy1py4__RXoK3Vn-YnS28H1wc40XxcpWCzj2da9aXN-JozbkrBZ_ywFRemrHjsQOwydgA5rWi2Mw1i7qHiqXz2Vug76fEiaE9zvitkBKWlxURM5d3jcIrHvw/file\n",
            "Resolving uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com (uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com (uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B2b96iIJRbsY1U-IjcnR_CTDitmbRE50UyX4dP--eQYqpaMae-BHXzqab_UHFbKLpXCxt_7EfZPirMPUe2wZNg0V_K5Di03YGfosm07zzep9VaAnxp8mqInvyUHfn5J3T6uOYWaf06avR9G885UI2rGa5GYg38JQtdgwpeKGJioYhSOHNlrgPAIKqd8tXNQkayl_kLYnJOsJqaZszpAWNFXmb3cq-fjUabFSDlhqTNSCKWuc2KediqqkzYoXu6Bijr1CnJT3P2KKYAIcUUcM7eWtJgubyQ9pebYDUEXpLYqx_6jtS5L4fHpiLI9AoORFKwVFkWjmH_dQMRVne7-jYQ3C22q1NTkpPISznQKhrm6zZT1uuXLA2HoCr7nra11kZlsjuvGlxCT-fOv1ottN5hrkbcdsEJH127tdNqPp81jXKQ/file [following]\n",
            "--2023-02-13 10:51:16--  https://uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com/cd/0/inline2/B2b96iIJRbsY1U-IjcnR_CTDitmbRE50UyX4dP--eQYqpaMae-BHXzqab_UHFbKLpXCxt_7EfZPirMPUe2wZNg0V_K5Di03YGfosm07zzep9VaAnxp8mqInvyUHfn5J3T6uOYWaf06avR9G885UI2rGa5GYg38JQtdgwpeKGJioYhSOHNlrgPAIKqd8tXNQkayl_kLYnJOsJqaZszpAWNFXmb3cq-fjUabFSDlhqTNSCKWuc2KediqqkzYoXu6Bijr1CnJT3P2KKYAIcUUcM7eWtJgubyQ9pebYDUEXpLYqx_6jtS5L4fHpiLI9AoORFKwVFkWjmH_dQMRVne7-jYQ3C22q1NTkpPISznQKhrm6zZT1uuXLA2HoCr7nra11kZlsjuvGlxCT-fOv1ottN5hrkbcdsEJH127tdNqPp81jXKQ/file\n",
            "Reusing existing connection to uc4cda363aacd537ace6c889b135.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 477487184 (455M) [application/x-gtar]\n",
            "Saving to: ‘data/binary/wmt16_en_de/wmt16_en_de.preprocessed.tgz’\n",
            "\n",
            "data/binary/wmt16_e 100%[===================>] 455.37M   124MB/s    in 3.7s    \n",
            "\n",
            "2023-02-13 10:51:20 (124 MB/s) - ‘data/binary/wmt16_en_de/wmt16_en_de.preprocessed.tgz’ saved [477487184/477487184]\n",
            "\n",
            "./\n",
            "./train.en-de.en.idx\n",
            "./valid.en-de.en.bin\n",
            "./valid.en-de.en.idx\n",
            "./test.en-de.en.bin\n",
            "./test.en-de.en.idx\n",
            "./train.en-de.de.bin\n",
            "./train.en-de.de.idx\n",
            "./valid.en-de.de.bin\n",
            "./valid.en-de.de.idx\n",
            "./test.en-de.de.bin\n",
            "./test.en-de.de.idx\n",
            "./dict.en.txt\n",
            "./dict.de.txt\n",
            "./train.en-de.en.bin\n"
          ]
        }
      ],
      "source": [
        "!bash configs/wmt14.en-de/get_preprocessed.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbTWn0m6i1kB"
      },
      "source": [
        "### 0.3 Inspect the search space\n",
        "\n",
        "Look at the search space config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYh_S1gh1CY",
        "outputId": "141cb068-3e57-4e5e-fe63-469f9e805cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# model\n",
            "arch: transformersuper_wmt_en_de\n",
            "share-all-embeddings: True\n",
            "max-tokens: 4096\n",
            "data: data/binary/wmt16_en_de\n",
            "\n",
            "# training settings\n",
            "optimizer: adam\n",
            "adam-betas: (0.9, 0.98)\n",
            "clip-norm: 0.0\n",
            "weight-decay: 0.0\n",
            "dropout: 0.3\n",
            "attention-dropout: 0.1\n",
            "criterion: label_smoothed_cross_entropy\n",
            "label-smoothing: 0.1\n",
            "\n",
            "ddp-backend: no_c10d\n",
            "fp16: True\n",
            "\n",
            "# warmup from warmup-init-lr to max-lr (warmup-updates steps); then cosine anneal to lr (max-update - warmup-updates steps)\n",
            "update-freq: 16\n",
            "max-update: 40000\n",
            "warmup-updates: 10000\n",
            "lr-scheduler: cosine\n",
            "warmup-init-lr: 1e-7\n",
            "max-lr: 0.001\n",
            "lr: 1e-7\n",
            "lr-shrink: 1\n",
            "\n",
            "# logging\n",
            "keep-last-epochs: 20\n",
            "save-interval: 10\n",
            "validate-interval: 10\n",
            "\n",
            "# SuperTransformer configs\n",
            "encoder-embed-dim: 640\n",
            "decoder-embed-dim: 640\n",
            "\n",
            "encoder-ffn-embed-dim: 3072\n",
            "decoder-ffn-embed-dim: 3072\n",
            "\n",
            "encoder-layers: 6\n",
            "decoder-layers: 6\n",
            "\n",
            "encoder-attention-heads: 8\n",
            "decoder-attention-heads: 8\n",
            "\n",
            "qkv-dim: 512\n",
            "\n",
            "# SubTransformers search space\n",
            "encoder-embed-choice: [640, 512]\n",
            "decoder-embed-choice: [640, 512]\n",
            "\n",
            "encoder-ffn-embed-dim-choice: [3072, 2048, 1024]\n",
            "decoder-ffn-embed-dim-choice: [3072, 2048, 1024]\n",
            "\n",
            "encoder-layer-num-choice: [6]\n",
            "decoder-layer-num-choice: [6, 5, 4, 3, 2, 1]\n",
            "\n",
            "encoder-self-attention-heads-choice: [8, 4]\n",
            "decoder-self-attention-heads-choice: [8, 4]\n",
            "decoder-ende-attention-heads-choice: [8, 4]\n",
            "\n",
            "# for arbitrary encoder decoder attention. -1 means attending to last one encoder layer\n",
            "# 1 means last two encoder layers, 2 means last three encoder layers\n",
            "decoder-arbitrary-ende-attn-choice: [-1, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "!cat configs/wmt14.en-de/supertransformer/space0.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNEe-krdjT55"
      },
      "source": [
        "`# SubTransformers search space` marks the search space for NAS, that defines the possible values taken by each Transformer hyperparameter.\n",
        "\n",
        "### 1. Supernet training\n",
        "\n",
        "A typical challenge in the NAS framework is to develop a **performance estimator** that can efficiently compute the accuracy of a candidate architecture. The naive approach of training candidate architectures from scratch to convergence and then evaluating on the validation set is prohibitively expensive given the large search space for all possible candidate architectures.\n",
        "\n",
        "HAT's performance estimator is based on weight-sharing via a Supernet. The supernet is the largest model in the search space (marked by `# SuperTransformer configs` in the previous config file).\n",
        "\n",
        "\n",
        "The Supernet is trained with the following steps:  \n",
        "1. sample a candidate architecture randomly from the search space\n",
        "2. train the sampled architecture by extracting the common portion of weights (subnet extraction) from different layers in the Supernet (i.e., by weight sharing) for one training step on the task\n",
        "3. repeat steps 1 and 2 until the training budget is exhausted. \n",
        "\n",
        "Once the Supernet training is complete, we can obtain a quick accuracy estimate for a candidate architecture (i.e. subnetwork) by extracting its shared weights from the Supernet and evaluating on the validation set.\n",
        "\n",
        "Let us understand how subnet extraction work via ``nn.Linear`` layer. As shown below, assume a linear layer in Supernet has 640 input features and 1024 output features ($1024\\times 640$). Say, the same linear layer in subnet has only 512 input features and 768 output features ($768\\times 512$). The linear layer weights for the subnet can be constructed by extracting the first 512 columns and first 768 rows from the corresponding weights of the supernet.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NRi9tF_LbAA5oZduxjn_lFCU_1eQMzHy\" alt=\"HAT generated architectures\" width=\"300\"/>\n",
        "\n",
        "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
        "\n",
        "\n",
        "Here's a sample implementation of ``nn.LinearSuper`` that generalizes ``nn.Linear``:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TL0xlhKRjQsf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LinearSuper(nn.Linear): # inherit from nn.Linear\n",
        "    def __init__(self, super_in_dim, super_out_dim, bias=True, uniform_=None, non_linear='linear'):\n",
        "        super().__init__(super_in_dim, super_out_dim, bias=bias)\n",
        "\n",
        "        # super_in_dim and super_out_dim indicate the largest network!\n",
        "        self.super_in_dim = super_in_dim\n",
        "        self.super_out_dim = super_out_dim\n",
        "\n",
        "        # input_dim and output_dim indicate the current sampled size\n",
        "        self.sample_in_dim = None\n",
        "        self.sample_out_dim = None\n",
        "\n",
        "        self.samples = {}\n",
        "\n",
        "        self._reset_parameters(bias, uniform_, non_linear)\n",
        "        self.profiling = False\n",
        "\n",
        "    def profile(self, mode=True):\n",
        "        self.profiling = mode\n",
        "\n",
        "    def sample_parameters(self, resample=False):\n",
        "        if self.profiling or resample:\n",
        "            return self._sample_parameters()\n",
        "        return self.samples\n",
        "\n",
        "    def _reset_parameters(self, bias, uniform_, non_linear):\n",
        "        nn.init.xavier_uniform_(self.weight) if uniform_ is None else uniform_(\n",
        "            self.weight, non_linear=non_linear)\n",
        "        if bias:\n",
        "            nn.init.constant_(self.bias, 0.)\n",
        "\n",
        "    def set_sample_config(self, sample_in_dim, sample_out_dim):\n",
        "        self.sample_in_dim = sample_in_dim\n",
        "        self.sample_out_dim = sample_out_dim\n",
        "\n",
        "        self._sample_parameters()\n",
        "\n",
        "    def _sample_parameters(self):\n",
        "        self.samples['weight'] = sample_weight(self.weight, self.sample_in_dim, self.sample_out_dim)\n",
        "        self.samples['bias'] = self.bias\n",
        "        if self.bias is not None:\n",
        "            self.samples['bias'] = sample_bias(self.bias, self.sample_out_dim)\n",
        "        return self.samples\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.sample_parameters()\n",
        "        return F.linear(x, self.samples['weight'], self.samples['bias'])\n",
        "\n",
        "    def calc_sampled_param_num(self):\n",
        "        assert 'weight' in self.samples.keys()\n",
        "        weight_numel = self.samples['weight'].numel() #Returns the total number of elements in the input tensor.\n",
        "\n",
        "        if self.samples['bias'] is not None:\n",
        "            bias_numel = self.samples['bias'].numel()\n",
        "        else:\n",
        "            bias_numel = 0\n",
        "\n",
        "        return weight_numel + bias_numel\n",
        "\n",
        "# weight extraction for subnet\n",
        "def sample_weight(weight, sample_in_dim, sample_out_dim):\n",
        "    sample_weight = weight[:, :sample_in_dim] # extract first `sample_in_dim` columns\n",
        "    sample_weight = sample_weight[:sample_out_dim, :] # extract first `sample_out_dim` columns\n",
        "\n",
        "    return sample_weight\n",
        "\n",
        "# bias extraction for subnet\n",
        "def sample_bias(bias, sample_out_dim):\n",
        "    sample_bias = bias[:sample_out_dim] # extract first `sample_out_dim` numbers\n",
        "\n",
        "    return sample_bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8HeBsYvie4"
      },
      "source": [
        "Let us construct the linear layer for Supernet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yePG_jyWuwDr",
        "outputId": "417a373c-c602-4bc3-9770-b5a7fb80b3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supernet: weight shape =  torch.Size([1024, 640])\n",
            "Supernet: bias shape =  torch.Size([1024])\n"
          ]
        }
      ],
      "source": [
        "linearlayer_supernet = LinearSuper(super_in_dim=640, super_out_dim=1024)\n",
        "# print the shape of weight matrix\n",
        "print(\"Supernet: weight shape = \", linearlayer_supernet.weight.shape)\n",
        "# print the shape of bias matrix\n",
        "print(\"Supernet: bias shape = \", linearlayer_supernet.bias.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqAC3fdjwY88"
      },
      "source": [
        "To extract the **subnet** weights, we use `set_sample_config()` function specifying the input and the output features as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehOQ_aZIwp5K",
        "outputId": "1605f6b8-3708-45d5-a0f8-5370822ef6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subnet: weight shape =  torch.Size([768, 512])\n",
            "Subnet: bias shape =  torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "linearlayer_supernet.set_sample_config(sample_in_dim=512, sample_out_dim=768)\n",
        "# print the shape of weight matrix\n",
        "print(\"Subnet: weight shape = \", linearlayer_supernet.samples['weight'].shape)\n",
        "# print the shape of bias matrix\n",
        "print(\"Subnet: bias shape = \", linearlayer_supernet.samples['bias'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29_b-HMYxOln"
      },
      "source": [
        "Refer to `fairseq/modules` in [HAT's repo](https://github.com/mit-han-lab/hardware-aware-transformers/tree/master/fairseq/modules) to see subnet extraction implementation for other Trasnformer layers, e.g., `multihead_attention_super` (self-attention), `embedding_super` (embedding layer).\n",
        "\n",
        "## A Toy Example\n",
        "Let us train a supernet for few steps now (change `max-tokens`, `max-update`, `update-freq` for full training). Before running the following command, change the line 198 of `fairseq/modules/multihead_attention_super.py` from `q *= self.scaling` to `q = q *self.scaling` (to avoid error: `RuntimeError: Output 0 of SplitBackward0 is a view and is being modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpIr3MUHyIus",
        "outputId": "ff6bf1d3-8775-4975-e576-70e7b1016d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Configs: Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=0.0, configs='configs/wmt14.en-de/supertransformer/space0.yml', cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, latcpu=False, latgpu=False, latiter=300, latsilent=False, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=5, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_flops=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='baseline/supernet', save_interval=10, save_interval_updates=5, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, sub_configs=None, t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='baseline/supernet/tensorboard', threshold_loss_scale=None, train_subset='train', train_subtransformer=False, unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=10, validate_subtransformer=False, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=10000, weight_decay=0.0)\n",
            "| [en] dictionary: 32768 types\n",
            "| [de] dictionary: 32768 types\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
            "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
            "| Fallback to xavier initializer\n",
            "| Model: transformersuper_wmt_en_de \n",
            "| Criterion: LabelSmoothedCrossEntropyCriterion\n",
            " \n",
            "\n",
            "\t\tWARNING!!! Training SuperTransformer\n",
            "\n",
            "\n",
            "| SuperTransformer Arch: TransformerSuperModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ") \n",
            "\n",
            "| SuperTransofmer model size (without embedding weights): 70900992\n",
            "| Embedding layer size: 20971520 \n",
            "\n",
            "| Training on 1 GPUs\n",
            "| Max tokens per GPU = 4096 and max sentences per GPU = None \n",
            "\n",
            "| no existing checkpoint found baseline/supernet/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.en\n",
            "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.de\n",
            "| data/binary/wmt16_en_de train en-de 4500966 examples\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| epoch 001:   0% 0/2376 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| WARNING: overflow detected, setting loss scale to: 64.0\n",
            "| epoch 001:   0% 1/2376 [00:03<2:29:07,  3.77s/it]| WARNING: overflow detected, setting loss scale to: 32.0\n",
            "| epoch 001:   0% 2/2376 [00:05<1:51:32,  2.82s/it]| WARNING: overflow detected, setting loss scale to: 16.0\n",
            "| epoch 001:   0% 3/2376 [00:07<1:36:46,  2.45s/it]| WARNING: overflow detected, setting loss scale to: 8.0\n",
            "| epoch 001:   0% 4/2376 [00:10<1:34:25,  2.39s/it]/content/hardware-aware-transformers/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "| epoch 001:   0% 8/2376 [00:19<1:26:18,  2.19s/it, loss=15.691, wps=3476, num_updates=4, lr=4.9996e-07]\n",
            "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:16,  1.71it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  13% 4/30 [00:00<00:03,  6.61it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  23% 7/30 [00:00<00:02, 10.39it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  30% 9/30 [00:00<00:01, 12.38it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  37% 11/30 [00:01<00:01, 14.06it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  47% 14/30 [00:01<00:01, 15.99it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  57% 17/30 [00:01<00:00, 17.40it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  63% 19/30 [00:01<00:00, 17.66it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  70% 21/30 [00:01<00:00, 18.05it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  77% 23/30 [00:01<00:00, 18.52it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  83% 25/30 [00:01<00:00, 18.27it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  90% 27/30 [00:01<00:00, 18.25it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  97% 29/30 [00:02<00:00, 18.25it/s]\u001b[A\n",
            "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 15.627 | nll_loss 15.617 | ppl 50240.96 | num_updates 5 | largest_arbitrary1_loss 15.627 | largest_arbitrary1_nll_loss 15.617\n",
            "\n",
            "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:15,  1.92it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  17% 5/30 [00:00<00:02,  9.57it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  30% 9/30 [00:00<00:01, 15.95it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  47% 14/30 [00:00<00:00, 22.87it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  63% 19/30 [00:00<00:00, 28.50it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  80% 24/30 [00:01<00:00, 31.95it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  93% 28/30 [00:01<00:00, 33.49it/s]\u001b[A\n",
            "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 20.431 | nll_loss 20.423 | ppl 1406180.60 | num_updates 5 | smallest_arbitrary1_loss 20.431 | smallest_arbitrary1_nll_loss 20.423\n",
            "| saved checkpoint baseline/supernet/checkpoint_1_5.pt (epoch 1 @ 5 updates) (writing took 11.117864608764648 seconds)\n",
            "| epoch 001 | loss 15.911 | nll_loss 15.905 | ppl 61369.00 | wps 3485 | ups 0 | wpb 59097.200 | bsz 1984.000 | num_updates 5 | lr 5.9995e-07 | gnorm 4.842 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 85 | train_wall 18\n",
            "| Done training in 65.4 seconds\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p baseline/supernet # stores supernet checkpoint\n",
        "!python -B train.py \\\n",
        "            --configs=configs/wmt14.en-de/supertransformer/space0.yml \\\n",
        "            --save-dir baseline/supernet \\\n",
        "            --no-epoch-checkpoints \\\n",
        "            --max-update 5 \\\n",
        "            --save-interval-updates 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iWJQIsg3SEO"
      },
      "source": [
        "The best supernet checkpoint can be accessed at `baseline/supernet/checkpoint_best.pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwOVTX9z1owH"
      },
      "source": [
        "### 2. Collect hardware latency datasets\n",
        "\n",
        "In the next step, we will generate the **hardware latency datasets**, which will be subsequently used to train a latency prediction model. This step will sample architectures from the search space and measure the latency of the architecture on the target hardware.\n",
        "\n",
        "Create a small dataset by running the following command (remove `--lat-dataset-size 25` to generate the full dataset):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZVqgz4K2KeT",
        "outputId": "e82c4c1a-ffa2-496f-ee27-76bdbdeb8e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.0, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=25, configs='configs/wmt14.en-de/latency_dataset/gpu_titanxp.yml', cpu=False, criterion='cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4, 2], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024, 512], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4, 2], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.1, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024, 512], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4, 2], find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=-1, lat_dataset_path='baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv', lat_dataset_size=25, latcpu=False, latgpu=True, latiter=20, latsilent=True, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='nag', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir=None, save_interval=1, save_interval_updates=0, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, vocab_original_scaling=False, warmup_updates=0, weight_decay=0.0)\n",
            "| [en] dictionary: 32768 types\n",
            "| [de] dictionary: 32768 types\n",
            "| Fallback to xavier initializer\n",
            "TransformerSuperModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Measuring model latency on GPU for dataset generation...\n",
            "0\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 177.98it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.411544024944305 ms; \t Std: 0.14899828720025418 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:02<00:00,  9.42it/s]\n",
            "[81.03724670410156, 83.6157455444336, 84.95209503173828, 87.40665435791016, 95.83654022216797, 109.37667083740234, 110.17919921875, 110.3556137084961, 110.97293090820312, 111.50723266601562, 112.90681457519531, 114.76608276367188, 117.34114837646484, 117.4487075805664, 122.109375, 123.56063842773438]\n",
            "Decoder latency for dataset generation: Mean: 105.83579349517822 ms; \t Std: 13.853732951636298 ms\n",
            "1\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 131.81it/s]\n",
            "Encoder latency for dataset generation: Mean: 7.387063950300217 ms; \t Std: 0.15016548686696587 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.84it/s]\n",
            "[186.30857849121094, 186.6588134765625, 186.89222717285156, 187.0391387939453, 189.116455078125, 191.52163696289062, 192.66432189941406, 193.6216278076172, 195.4773406982422, 196.06527709960938, 196.90293884277344, 197.0339813232422, 197.06092834472656, 200.4358673095703, 205.09622192382812, 252.82553100585938]\n",
            "Decoder latency for dataset generation: Mean: 197.1700553894043 ms; \t Std: 15.292143399237476 ms\n",
            "2\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 175.83it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.453968018293381 ms; \t Std: 0.1710225888243829 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 20.36it/s]\n",
            "[45.865089416503906, 46.83571243286133, 46.88438415527344, 46.93830490112305, 47.278079986572266, 47.34156799316406, 47.812320709228516, 48.283103942871094, 48.67171096801758, 48.681663513183594, 48.93145751953125, 49.19705581665039, 49.43318557739258, 50.25385665893555, 50.64089584350586, 51.55478286743164]\n",
            "Decoder latency for dataset generation: Mean: 48.41269826889038 ms; \t Std: 1.5122681240851004 ms\n",
            "3\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 176.88it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.593298017978668 ms; \t Std: 0.10503452884877318 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:03<00:00,  5.15it/s]\n",
            "[187.1165771484375, 187.2989501953125, 189.2024383544922, 189.27821350097656, 190.1834259033203, 190.9923858642578, 192.11468505859375, 192.16403198242188, 192.80691528320312, 193.33731079101562, 196.3700408935547, 197.49769592285156, 197.84259033203125, 201.65890502929688, 201.72274780273438, 202.32806396484375]\n",
            "Decoder latency for dataset generation: Mean: 193.86968612670898 ms; \t Std: 4.937093634661112 ms\n",
            "4\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 171.65it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.655045986175537 ms; \t Std: 0.29456080855649136 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:03<00:00,  5.27it/s]\n",
            "[154.13658142089844, 156.40042114257812, 156.9853515625, 157.82333374023438, 160.62258911132812, 161.07711791992188, 191.13189697265625, 196.9710693359375, 207.64364624023438, 208.52879333496094, 212.31295776367188, 213.1985626220703, 216.4369659423828, 216.6051483154297, 216.79641723632812, 217.6532745361328]\n",
            "Decoder latency for dataset generation: Mean: 190.2702579498291 ms; \t Std: 26.063443727571514 ms\n",
            "5\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 173.93it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.670296013355255 ms; \t Std: 0.2147512017618628 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.40it/s]\n",
            "[216.8837127685547, 217.19091796875, 218.60806274414062, 218.6559295654297, 220.56153869628906, 220.69436645507812, 221.2737579345703, 221.38064575195312, 225.37423706054688, 225.4459228515625, 227.48748779296875, 230.455322265625, 235.1266632080078, 236.28594970703125, 236.897216796875, 238.07289123535156]\n",
            "Decoder latency for dataset generation: Mean: 225.6496639251709 ms; \t Std: 7.26757887455495 ms\n",
            "6\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 160.11it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.988003969192505 ms; \t Std: 0.6823024888156334 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.27it/s]\n",
            "[221.93356323242188, 222.88038635253906, 223.2606964111328, 228.14352416992188, 228.3263397216797, 228.91488647460938, 230.61708068847656, 231.14341735839844, 232.1204833984375, 233.48672485351562, 237.59365844726562, 238.5285186767578, 240.7262725830078, 243.07093811035156, 245.23158264160156, 245.7722625732422]\n",
            "Decoder latency for dataset generation: Mean: 233.23439598083496 ms; \t Std: 7.568963942768719 ms\n",
            "7\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 162.52it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.9282360672950745 ms; \t Std: 0.9466389412245436 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.33it/s]\n",
            "[187.05206298828125, 190.41690063476562, 190.73651123046875, 193.30178833007812, 197.57395935058594, 198.1685791015625, 216.1076202392578, 219.96099853515625, 220.26412963867188, 252.79696655273438, 255.00233459472656, 256.6945495605469, 259.9376220703125, 260.495361328125, 264.5201721191406, 268.1141052246094]\n",
            "Decoder latency for dataset generation: Mean: 226.94647884368896 ms; \t Std: 30.548906338181617 ms\n",
            "8\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 183.95it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.23349004983902 ms; \t Std: 0.1563646349462059 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.46it/s]\n",
            "[216.2171173095703, 216.67715454101562, 217.0853729248047, 218.6444549560547, 219.90438842773438, 221.0713653564453, 221.2507781982422, 224.3329620361328, 224.37277221679688, 224.82269287109375, 225.2093505859375, 226.765380859375, 227.10365295410156, 230.05392456054688, 231.24838256835938, 234.855712890625]\n",
            "Decoder latency for dataset generation: Mean: 223.72596645355225 ms; \t Std: 5.298016299108231 ms\n",
            "9\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 173.46it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.483631998300552 ms; \t Std: 0.2126527887619351 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 20.93it/s]\n",
            "[43.93404769897461, 44.73177719116211, 44.74105453491211, 45.38572692871094, 45.52703857421875, 45.8581428527832, 46.011871337890625, 46.39945602416992, 46.57596969604492, 46.5843505859375, 46.75174331665039, 47.106048583984375, 49.0618896484375, 49.358848571777344, 50.45248031616211, 51.104095458984375]\n",
            "Decoder latency for dataset generation: Mean: 46.84903383255005 ms; \t Std: 2.0264451859475323 ms\n",
            "10\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 178.16it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.4431100487709045 ms; \t Std: 0.29991803637859976 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:02<00:00,  7.82it/s]\n",
            "[111.05177307128906, 112.18694305419922, 113.02665710449219, 113.14822387695312, 113.66812896728516, 114.36441802978516, 116.03862762451172, 116.96870422363281, 118.25190734863281, 118.82003021240234, 124.3367691040039, 131.32595825195312, 147.053466796875, 149.84217834472656, 150.94578552246094, 153.66348266601562]\n",
            "Decoder latency for dataset generation: Mean: 125.29331588745117 ms; \t Std: 15.309241318631463 ms\n",
            "11\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 135.93it/s]\n",
            "Encoder latency for dataset generation: Mean: 7.28277000784874 ms; \t Std: 0.3530373560707506 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:02<00:00,  9.91it/s]\n",
            "[79.52387237548828, 82.90589141845703, 84.15878295898438, 84.33900451660156, 85.13330841064453, 88.03036499023438, 102.891357421875, 103.45820617675781, 103.61650848388672, 105.00006103515625, 105.54163360595703, 108.99651336669922, 109.6943359375, 116.45257568359375, 118.76751708984375, 124.47695922851562]\n",
            "Decoder latency for dataset generation: Mean: 100.1866807937622 ms; \t Std: 13.823618460087127 ms\n",
            "12\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 182.65it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.3137039840221405 ms; \t Std: 0.0690872500786107 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.49it/s]\n",
            "[212.15667724609375, 213.8244171142578, 216.45928955078125, 216.8238067626953, 216.90623474121094, 221.40518188476562, 222.22618103027344, 222.36813354492188, 223.127685546875, 224.86630249023438, 225.87709045410156, 226.61868286132812, 228.36636352539062, 228.83941650390625, 230.09703063964844, 231.93382263183594]\n",
            "Decoder latency for dataset generation: Mean: 222.61851978302002 ms; \t Std: 5.804049443702551 ms\n",
            "13\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 183.42it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.338782042264938 ms; \t Std: 0.1653670832933184 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:01<00:00, 11.85it/s]\n",
            "[77.37225341796875, 79.75577545166016, 80.042236328125, 80.93135833740234, 81.0898208618164, 81.63442993164062, 82.5159683227539, 82.55010986328125, 82.585693359375, 82.95980834960938, 83.57478332519531, 85.88057708740234, 86.73490905761719, 88.06192016601562, 89.33948516845703, 90.5789794921875]\n",
            "Decoder latency for dataset generation: Mean: 83.47550678253174 ms; \t Std: 3.5668671889217864 ms\n",
            "14\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 179.86it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.363496005535126 ms; \t Std: 0.2345347413380823 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:05<00:00,  3.82it/s]\n",
            "[223.10464477539062, 226.79151916503906, 226.8344268798828, 226.9358673095703, 228.6037139892578, 229.14224243164062, 232.3639678955078, 232.98646545410156, 239.11917114257812, 241.8139190673828, 279.3941650390625, 296.68560791015625, 301.1440734863281, 302.66131591796875, 306.01129150390625, 323.21630859375]\n",
            "Decoder latency for dataset generation: Mean: 257.3005437850952 ms; \t Std: 35.43211124643128 ms\n",
            "15\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 168.13it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.7963279485702515 ms; \t Std: 0.26496580486093835 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:03<00:00,  5.08it/s]\n",
            "[187.3651580810547, 187.4451904296875, 188.9430389404297, 189.0079345703125, 189.25050354003906, 189.68377685546875, 190.37753295898438, 190.9759979248047, 193.75302124023438, 193.90921020507812, 195.9644775390625, 196.41094970703125, 197.74803161621094, 198.3751983642578, 199.6295623779297, 199.68150329589844]\n",
            "Decoder latency for dataset generation: Mean: 193.03256797790527 ms; \t Std: 4.2783059832012915 ms\n",
            "16\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 172.35it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.636121958494186 ms; \t Std: 0.3482853305522311 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:01<00:00, 11.65it/s]\n",
            "[80.93571472167969, 81.10387420654297, 81.4119644165039, 81.60157012939453, 81.7726058959961, 82.79065704345703, 83.46211242675781, 84.05443572998047, 84.78514862060547, 85.45894622802734, 86.00841522216797, 86.80038452148438, 87.75312042236328, 88.70912170410156, 90.92156982421875, 91.41651153564453]\n",
            "Decoder latency for dataset generation: Mean: 84.93663454055786 ms; \t Std: 3.3232391561029906 ms\n",
            "17\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 177.75it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.569758027791977 ms; \t Std: 0.1170965199771483 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.81it/s]\n",
            "[188.64151000976562, 190.26547241210938, 194.22344970703125, 195.17645263671875, 195.34681701660156, 197.6314239501953, 198.62258911132812, 199.42013549804688, 199.76515197753906, 200.71270751953125, 201.15455627441406, 202.20828247070312, 204.30044555664062, 206.574462890625, 214.10202026367188, 226.95770263671875]\n",
            "Decoder latency for dataset generation: Mean: 200.94394874572754 ms; \t Std: 8.978186493562857 ms\n",
            "18\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 127.85it/s]\n",
            "Encoder latency for dataset generation: Mean: 7.427907973527908 ms; \t Std: 0.18838018971409137 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:05<00:00,  3.94it/s]\n",
            "[222.80691528320312, 223.34918212890625, 228.26870727539062, 228.9872589111328, 229.75267028808594, 231.0720977783203, 231.8970947265625, 232.53245544433594, 233.79043579101562, 235.98150634765625, 238.20700073242188, 238.4547576904297, 251.9839630126953, 295.8704528808594, 300.6302795410156, 309.01861572265625]\n",
            "Decoder latency for dataset generation: Mean: 245.78771209716797 ms; \t Std: 27.801711839213034 ms\n",
            "19\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 176.16it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.619531989097595 ms; \t Std: 0.10699011974133271 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:01<00:00, 11.60it/s]\n",
            "[80.71932983398438, 81.39622497558594, 81.90262603759766, 82.72688293457031, 82.74736022949219, 83.83097839355469, 84.1398696899414, 84.52301025390625, 84.68653106689453, 85.1239013671875, 85.33507537841797, 86.81632232666016, 87.13196563720703, 88.69532775878906, 90.08128356933594, 90.08169555664062]\n",
            "Decoder latency for dataset generation: Mean: 84.99614906311035 ms; \t Std: 2.8156808178430124 ms\n",
            "20\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 174.82it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.6416140496730804 ms; \t Std: 0.1751718859103004 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:01<00:00, 19.74it/s]\n",
            "[47.41932678222656, 47.5997428894043, 48.88780975341797, 49.01683044433594, 49.19660949707031, 49.30419158935547, 49.35958480834961, 49.438655853271484, 50.05510330200195, 50.470848083496094, 50.59196853637695, 51.055809020996094, 51.89836883544922, 53.09183883666992, 53.13865661621094, 54.53619384765625]\n",
            "Decoder latency for dataset generation: Mean: 50.316346168518066 ms; \t Std: 1.9390672604281178 ms\n",
            "21\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 178.87it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.4600419998168945 ms; \t Std: 0.30210393617574866 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:02<00:00,  8.30it/s]\n",
            "[113.76073455810547, 115.87315368652344, 116.4800033569336, 116.79507446289062, 116.8340835571289, 117.26233673095703, 117.85011291503906, 119.73632049560547, 120.1710433959961, 121.2281265258789, 121.49964904785156, 123.28550720214844, 123.31619262695312, 125.4134750366211, 125.96387481689453, 126.060546875]\n",
            "Decoder latency for dataset generation: Mean: 120.09563970565796 ms; \t Std: 3.773809143583869 ms\n",
            "22\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 173.94it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.550027936697006 ms; \t Std: 0.5108549894491795 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:03<00:00,  6.62it/s]\n",
            "[117.61663818359375, 120.26675415039062, 122.5896987915039, 125.73353576660156, 154.6356201171875, 154.6854705810547, 154.82266235351562, 156.28953552246094, 157.27536010742188, 158.00729370117188, 164.0401611328125, 164.45333862304688, 164.6116485595703, 166.77743530273438, 170.7151336669922, 174.08551025390625]\n",
            "Decoder latency for dataset generation: Mean: 151.6628623008728 ms; \t Std: 18.306820802481774 ms\n",
            "23\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 137.09it/s]\n",
            "Encoder latency for dataset generation: Mean: 7.213757902383804 ms; \t Std: 1.6255730711085785 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:03<00:00,  6.28it/s]\n",
            "[148.844482421875, 149.97010803222656, 155.0213165283203, 155.35784912109375, 157.24957275390625, 157.27206420898438, 159.18739318847656, 159.361572265625, 159.38153076171875, 159.9319610595703, 160.04296875, 160.069091796875, 160.43011474609375, 160.93113708496094, 161.08639526367188, 166.53724670410156]\n",
            "Decoder latency for dataset generation: Mean: 158.16717529296875 ms; \t Std: 4.176341384937099 ms\n",
            "24\n",
            "Measuring encoder for dataset generation...\n",
            "100% 20/20 [00:00<00:00, 180.17it/s]\n",
            "Encoder latency for dataset generation: Mean: 5.324553996324539 ms; \t Std: 0.3192922070677719 ms\n",
            "Measuring decoder for dataset generation...\n",
            "100% 20/20 [00:04<00:00,  4.99it/s]\n",
            "[186.4048614501953, 189.31088256835938, 190.97999572753906, 191.09327697753906, 192.53042602539062, 194.28358459472656, 195.3602294921875, 199.24899291992188, 199.3482208251953, 199.48745727539062, 199.5248260498047, 201.08087158203125, 204.66893005371094, 204.68663024902344, 205.86495971679688, 211.54751586914062]\n",
            "Decoder latency for dataset generation: Mean: 197.83885383605957 ms; \t Std: 6.657691371023292 ms\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p baseline/genlatdata # to save latency dataset\n",
        "!CUDA_VISIBLE_DEVICES=0 \n",
        "!python latency_dataset.py \\\n",
        "        --configs=configs/wmt14.en-de/latency_dataset/gpu_titanxp.yml  \\\n",
        "        --lat-dataset-path baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv \\\n",
        "        --lat-dataset-size 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO5GuMtt3LZO"
      },
      "source": [
        "The latency dataset can be accessed at `baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI_57HA_3iBf"
      },
      "source": [
        "### 3. Train latency predictor \n",
        "\n",
        "After generating the latency dataset, we can train a latency predictor. HAT's predictor is based on a simple 2-layer MLP based regressor.\n",
        "\n",
        "Run the following command to train latency predictor (remove `--bsz 2` for full run):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep2AVvJw3hB4",
        "outputId": "de08910a-bcda-4199-8cf1-cffc098da3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(bsz=2, ckpt_path='baseline/latpred/wmt14.en-de_gpu_titanxp.pt', configs='configs/wmt14.en-de/latency_predictor/gpu_titanxp.yml', dataset_path=None, feature_dim=10, feature_norm=[640.0, 6.0, 2048.0, 6.0, 640.0, 6.0, 2048.0, 6.0, 6.0, 2.0], hidden_dim=400, hidden_layer_num=3, lat_dataset_path='baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv', lat_norm=200.0, lr=1e-05, train_steps=5000)\n",
            "latency_predictor.py:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  sample_x_tensor = torch.Tensor(sample_x)\n",
            "Validation loss at 0 steps: 1.3056495189666748\n",
            "Validation loss at 100 steps: 0.8925215005874634\n",
            "Validation loss at 200 steps: 0.3997454047203064\n",
            "Validation loss at 300 steps: 0.12640422582626343\n",
            "Validation loss at 400 steps: 0.08265182375907898\n",
            "Validation loss at 500 steps: 0.0819818377494812\n",
            "Validation loss at 600 steps: 0.07359975576400757\n",
            "Validation loss at 700 steps: 0.0808529183268547\n",
            "Validation loss at 800 steps: 0.0610981248319149\n",
            "Validation loss at 900 steps: 0.0539892241358757\n",
            "Validation loss at 1000 steps: 0.03898584097623825\n",
            "Validation loss at 1100 steps: 0.01953168958425522\n",
            "Validation loss at 1200 steps: 0.018018370494246483\n",
            "Validation loss at 1300 steps: 0.012528874911367893\n",
            "Validation loss at 1400 steps: 0.010474536567926407\n",
            "Validation loss at 1500 steps: 0.007149890065193176\n",
            "Validation loss at 1600 steps: 0.009642566554248333\n",
            "Validation loss at 1700 steps: 0.0077134487219154835\n",
            "Validation loss at 1800 steps: 0.008390761911869049\n",
            "Validation loss at 1900 steps: 0.008354205638170242\n",
            "Validation loss at 2000 steps: 0.007012019399553537\n",
            "Validation loss at 2100 steps: 0.007814869284629822\n",
            "Validation loss at 2200 steps: 0.007560174912214279\n",
            "Validation loss at 2300 steps: 0.0071643958799541\n",
            "Validation loss at 2400 steps: 0.006873669568449259\n",
            "Validation loss at 2500 steps: 0.006941450759768486\n",
            "Validation loss at 2600 steps: 0.007028753403574228\n",
            "Validation loss at 2700 steps: 0.0068847620859742165\n",
            "Validation loss at 2800 steps: 0.007325550075620413\n",
            "Validation loss at 2900 steps: 0.007724079303443432\n",
            "Validation loss at 3000 steps: 0.007033536210656166\n",
            "Validation loss at 3100 steps: 0.006400124169886112\n",
            "Validation loss at 3200 steps: 0.0056376405991613865\n",
            "Validation loss at 3300 steps: 0.006652181036770344\n",
            "Validation loss at 3400 steps: 0.0060179890133440495\n",
            "Validation loss at 3500 steps: 0.006122914608567953\n",
            "Validation loss at 3600 steps: 0.006443139165639877\n",
            "Validation loss at 3700 steps: 0.006376516539603472\n",
            "Validation loss at 3800 steps: 0.006185318809002638\n",
            "Validation loss at 3900 steps: 0.005397173576056957\n",
            "Validation loss at 4000 steps: 0.005643730517476797\n",
            "Validation loss at 4100 steps: 0.005813159514218569\n",
            "Validation loss at 4200 steps: 0.006131582427769899\n",
            "Validation loss at 4300 steps: 0.006625044159591198\n",
            "Validation loss at 4400 steps: 0.006145573686808348\n",
            "Validation loss at 4500 steps: 0.005920331459492445\n",
            "Validation loss at 4600 steps: 0.006262652575969696\n",
            "Validation loss at 4700 steps: 0.005857594311237335\n",
            "Validation loss at 4800 steps: 0.005909569561481476\n",
            "Validation loss at 4900 steps: 0.006150389090180397\n",
            "Predicted latency: tensor([0.2932, 0.7893, 0.5451])\n",
            "Real latency: (0.27978980109095575, 0.7860644511878491, 0.5562366876006126)\n",
            "Loss: 0.00010462723003001884\n",
            "RMSE: 2.045747756958008\n",
            "MAPD: 0.024003110826015472\n",
            "Latency predictor training finished\n",
            "Example config: {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 5, 'decoder_ffn_embed_dim': [2048, 3072, 3072, 3072, 1024], 'decoder_self_attention_heads': [4, 8, 8, 4, 4], 'decoder_ende_attention_heads': [4, 8, 8, 4, 4], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, 1]}}\n",
            "Example latency: 239.53495025634766\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p baseline/latpred # stores latency predictor checkpoint\n",
        "!python latency_predictor.py \\\n",
        "        --configs=configs/wmt14.en-de/latency_predictor/gpu_titanxp.yml \\\n",
        "        --feature-norm 640 6 2048 6 640 6 2048 6 6 2 \\\n",
        "        --feature-dim 10 \\\n",
        "        --lat-dataset-path baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv \\\n",
        "        --ckpt-path baseline/latpred/wmt14.en-de_gpu_titanxp.pt \\\n",
        "        --bsz 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_rDgZ_34pr1"
      },
      "source": [
        "The latency predictor can be accessed at `baseline/latpred/wmt14.en-de_gpu_titanxp.pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7SsMHSn5jNm"
      },
      "source": [
        "### 4. Evolutionary search\n",
        "\n",
        "Now, we have a latency and performance predictor to quickly get latency and performance of a candidate architecture. We can perform evolutionary search that also takes latency constraint (less than 200 milliseconds) to identify efficient architecture that maximizes the BLEU score, while satisfying the constraint.\n",
        "\n",
        "Run the following command to start the search (remove `--evo-iter 1 --parent-size 2 --mutation-size 2 --crossover-size 2 --population-size 6` for full search):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiMFMII_4u7H",
        "outputId": "f7ecb006-4579-4ddc-e74b-9c6516b292a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, ckpt_path='baseline/latpred/wmt14.en-de_gpu_titanxp.pt', clip_norm=0.0, configs='configs/wmt14.en-de/supertransformer/space0.yml', cpu=False, criterion='label_smoothed_cross_entropy', crossover_size=2, curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4], evo_configs='configs/wmt14.en-de/evo_search/wmt14ende_titanxp.yml', evo_iter=1, feature_norm=[640.0, 6.0, 2048.0, 6.0, 640.0, 6.0, 2048.0, 6.0, 6.0, 2.0], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, lat_norm=200.0, latency_constraint=200.0, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=40000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', mutation_prob=0.3, mutation_size=2, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', parent_size=2, path=None, pdb=False, population_size=6, prefix_size=0, print_alignment=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='baseline/supernet/checkpoint_best.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints/wmt14.en-de/supertransformer/space0', save_interval=10, save_interval_updates=0, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='checkpoints/wmt14.en-de/supertransformer/space0/tensorboard', threshold_loss_scale=None, train_subset='train', unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_cnt_max=1000000000.0, valid_subset='valid', validate_interval=10, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=10000, weight_decay=0.0, write_config_path='baseline/evosearch/wmt14.en-de_gpu_titanxp.yml')\n",
            "| [en] dictionary: 32768 types\n",
            "| [de] dictionary: 32768 types\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
            "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
            "| Fallback to xavier initializer\n",
            "TransformerSuperModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "| loaded checkpoint baseline/supernet/checkpoint_best.pt (epoch 1 @ 5 updates)\n",
            "| loading train data for epoch 1\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
            "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
            "| Start Iteration 0:\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| epoch 001 | valid on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| Iteration 0, Lowest loss: 15.502814838989167\n",
            "| Config for lowest loss model: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 2048, 2048, 3072, 1024], 'encoder_self_attention_heads': [4, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [2048, 2048, 1024, 2048, 3072, 1024], 'decoder_self_attention_heads': [4, 4, 8, 4, 4, 4], 'decoder_ende_attention_heads': [8, 8, 4, 4, 4, 4], 'decoder_arbitrary_ende_attn': [1, 1, -1, 2, 1, -1]}}\n",
            "| Predicted latency for lowest loss model: 197.7116346359253\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p baseline/evosearch # to store best architecture config\n",
        "!CUDA_VISIBLE_DEVICES=0 \n",
        "!python evo_search.py \\\n",
        "        --configs=configs/wmt14.en-de/supertransformer/space0.yml \\\n",
        "        --evo-configs=configs/wmt14.en-de/evo_search/wmt14ende_titanxp.yml \\\n",
        "        --restore-file baseline/supernet/checkpoint_best.pt \\\n",
        "        --ckpt-path baseline/latpred/wmt14.en-de_gpu_titanxp.pt \\\n",
        "        --feature-norm 640 6 2048 6 640 6 2048 6 6 2 \\\n",
        "        --write-config-path baseline/evosearch/wmt14.en-de_gpu_titanxp.yml \\\n",
        "        --evo-iter 1 \\\n",
        "        --parent-size 2 \\\n",
        "        --mutation-size 2 \\\n",
        "        --crossover-size 2 \\\n",
        "        --population-size 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjY5_1fA7rFR"
      },
      "source": [
        "The config for efficient architecture can be found at: `baseline/evosearch/wmt14.en-de_gpu_titanxp.yml`.\n",
        "\n",
        "Let us take a look at this config:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMEyd5qk7xg9",
        "outputId": "a1aba9bc-ad63-4a69-ec38-0e9bb404b22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder-embed-dim-subtransformer: 640\n",
            "decoder-embed-dim-subtransformer: 512\n",
            "\n",
            "encoder-ffn-embed-dim-all-subtransformer: [3072, 3072, 2048, 2048, 3072, 1024]\n",
            "decoder-ffn-embed-dim-all-subtransformer: [2048, 2048, 1024, 2048]\n",
            "\n",
            "encoder-layer-num-subtransformer: 6\n",
            "decoder-layer-num-subtransformer: 4\n",
            "\n",
            "encoder-self-attention-heads-all-subtransformer: [4, 8, 8, 8, 8, 4]\n",
            "decoder-self-attention-heads-all-subtransformer: [4, 4, 8, 4]\n",
            "decoder-ende-attention-heads-all-subtransformer: [8, 8, 4, 4]\n",
            "\n",
            "decoder-arbitrary-ende-attn-all-subtransformer: [1, 1, -1, 2]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cat baseline/evosearch/wmt14.en-de_gpu_titanxp.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKfIpQbS8XP1"
      },
      "source": [
        "### 5. Train efficient architecture from scratch\n",
        "\n",
        "Now, we have the efficient architecture. All that is left is to train the architecture from scratch (random initialization) to convergence. The trained architecture should be ideal for deployment in the target hardware.\n",
        "\n",
        "Run the following command to train the efficient model (remove `---max-update 5 --save-interval-updates 5` for full training):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92sizj7x8vWj",
        "outputId": "a044d77d-2fef-469e-e6d0-c1566bc291c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Configs: Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=0.0, configs='baseline/evosearch/wmt14.en-de_gpu_titanxp.yml', cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=[1, 1, -1, 2], decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[512, 256, 128], decoder_embed_dim=640, decoder_embed_dim_subtransformer=512, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=[8, 8, 4, 4], decoder_ende_attention_heads_choice=[16, 8, 4, 2, 1], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=[2048, 2048, 1024, 2048], decoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[7, 6, 5, 4, 3, 2], decoder_layer_num_subtransformer=4, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=[4, 4, 8, 4], decoder_self_attention_heads_choice=[16, 8, 4, 2, 1], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[512, 256, 128], encoder_embed_dim=640, encoder_embed_dim_subtransformer=640, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=[3072, 3072, 2048, 2048, 3072, 1024], encoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], encoder_layer_num_choice=[7, 6, 5, 4, 3, 2], encoder_layer_num_subtransformer=6, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=[4, 8, 8, 8, 8, 4], encoder_self_attention_heads_choice=[16, 8, 4, 2, 1], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, latcpu=False, latgpu=False, latiter=300, latsilent=False, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=5, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_flops=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='baseline/effnet', save_interval=10, save_interval_updates=5, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, sub_configs='configs/wmt14.en-de/subtransformer/common.yml', t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='baseline/effnet/tensorboard', threshold_loss_scale=None, train_subset='train', train_subtransformer=True, unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=10, validate_subtransformer=False, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\n",
            "| [en] dictionary: 32768 types\n",
            "| [de] dictionary: 32768 types\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
            "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
            "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
            "| Fallback to xavier initializer\n",
            "| Model: transformersuper_wmt_en_de \n",
            "| Criterion: LabelSmoothedCrossEntropyCriterion\n",
            " \n",
            "\n",
            "\t\tWARNING!!! Training one single SubTransformer\n",
            "\n",
            "\n",
            "| SubTransformer Arch: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 2048, 2048, 3072, 1024], 'encoder_self_attention_heads': [4, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [2048, 2048, 1024, 2048], 'decoder_self_attention_heads': [4, 4, 8, 4], 'decoder_ende_attention_heads': [8, 8, 4, 4], 'decoder_arbitrary_ende_attn': [1, 1, -1, 2]}} \n",
            "\n",
            "| SubTransformer size (without embedding weights): 42551808\n",
            "| Embedding layer size: 16777216 \n",
            "\n",
            "| Training on 1 GPUs\n",
            "| Max tokens per GPU = 4096 and max sentences per GPU = None \n",
            "\n",
            "| no existing checkpoint found baseline/effnet/checkpoint_last.pt\n",
            "| loading train data for epoch 0\n",
            "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.en\n",
            "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.de\n",
            "| data/binary/wmt16_en_de train en-de 4500966 examples\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| epoch 001:   0% 0/2376 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "| WARNING: overflow detected, setting loss scale to: 64.0\n",
            "| epoch 001:   0% 1/2376 [00:03<2:37:23,  3.98s/it]| WARNING: overflow detected, setting loss scale to: 32.0\n",
            "| epoch 001:   0% 2/2376 [00:06<1:52:27,  2.84s/it]| WARNING: overflow detected, setting loss scale to: 16.0\n",
            "| epoch 001:   0% 3/2376 [00:08<1:37:47,  2.47s/it]| WARNING: overflow detected, setting loss scale to: 8.0\n",
            "| epoch 001:   0% 4/2376 [00:10<1:37:53,  2.48s/it]/content/hardware-aware-transformers/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "| epoch 001:   0% 8/2376 [00:19<1:30:04,  2.28s/it, loss=15.491, wps=3454, num_updates=4, lr=1.0999e-06] \n",
            "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:18,  1.60it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  13% 4/30 [00:00<00:04,  6.41it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  23% 7/30 [00:00<00:02, 10.96it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  33% 10/30 [00:00<00:01, 14.75it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  43% 13/30 [00:01<00:00, 17.90it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  53% 16/30 [00:01<00:00, 20.18it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  63% 19/30 [00:01<00:00, 22.18it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  73% 22/30 [00:01<00:00, 23.75it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  83% 25/30 [00:01<00:00, 24.28it/s]\u001b[A\n",
            "| epoch 001 | validate on 'valid' subset:  93% 28/30 [00:01<00:00, 24.01it/s]\u001b[A\n",
            "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 15.490 | nll_loss 15.480 | ppl 45711.87 | num_updates 5 | subtransformer_loss 15.490 | subtransformer_nll_loss 15.480\n",
            "| saved checkpoint baseline/effnet/checkpoint_1_5.pt (epoch 1 @ 5 updates) (writing took 11.756999254226685 seconds)\n",
            "| epoch 001 | loss 15.490 | nll_loss 15.480 | ppl 45704.98 | wps 3496 | ups 0 | wpb 59097.200 | bsz 1984.000 | num_updates 5 | lr 1.34987e-06 | gnorm 3.968 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 85 | train_wall 19\n",
            "| Done training in 66.5 seconds\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p baseline/effnet # stores the checkpoint for efficient model\n",
        "!python -B train.py \\\n",
        "            --configs=baseline/evosearch/wmt14.en-de_gpu_titanxp.yml \\\n",
        "            --save-dir baseline/effnet \\\n",
        "            --sub-configs=configs/wmt14.en-de/subtransformer/common.yml \\\n",
        "            --no-epoch-checkpoints \\\n",
        "            --max-update 5 \\\n",
        "            --save-interval-updates 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB1gRtZs-WdK"
      },
      "source": [
        "The checkpoint for the efficient model can be accessed at `baseline/effnet/checkpoint_best.pt`.\n",
        "\n",
        "## 5.1 Get performance of the efficient architecture\n",
        "\n",
        "Change the line 81 in `fairseq/search.py` from `torch.div(self.indices_buf, vocab_size, out=self.beams_buf)` to `self.beams_buf = torch.div(self.indices_buf, vocab_size).type_as(self.beams_buf)`. Otherwise, you will get the error `RuntimeError: result type Float can't be cast to the desired output type Long`.\n",
        "\n",
        "Get the BLEU score on the validation set by running the following command (expect the results to be 0 as we only did a trial run of HAT pipeline):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUyFULVv-lwQ",
        "outputId": "9fdf51aa-fb66-4362-eaeb-e6f050c9df63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerSuperModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "baseline/effnet/checkpoint_best.pt\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 96% 23/24 [04:04<00:13, 13.33s/it, wps=2418]/content/hardware-aware-transformers/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [56, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
            "  torch.topk(\n",
            "Evaluate Normal BLEU score!\n",
            "Namespace(ignore_case=False, order=4, ref='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_valid_gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_valid_gen.out.sys')\n",
            "BLEU4 = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=7.455, syslen=478941, reflen=64248)\n"
          ]
        }
      ],
      "source": [
        "!bash configs/wmt14.en-de/test.sh baseline/effnet/checkpoint_best.pt baseline/evosearch/wmt14.en-de_gpu_titanxp.yml normal 0 valid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKgJIddPDKm2"
      },
      "source": [
        "Get the BLEU score on the test set by running the following command (expect the results to be 0 as we only did a trial run of HAT pipeline):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkLFdJDVDNRx",
        "outputId": "68a550a1-1538-4a9c-fb12-a2c3837742f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerSuperModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
            "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
            "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
            "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "baseline/effnet/checkpoint_best.pt\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 96% 23/24 [04:11<00:13, 13.16s/it, wps=2357]/content/hardware-aware-transformers/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [59, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
            "  torch.topk(\n",
            "Evaluate Normal BLEU score!\n",
            "Namespace(ignore_case=False, order=4, ref='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_test_gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_test_gen.out.sys')\n",
            "BLEU4 = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=7.533, syslen=485842, reflen=64496)\n"
          ]
        }
      ],
      "source": [
        "!bash configs/wmt14.en-de/test.sh baseline/effnet/checkpoint_best.pt baseline/evosearch/wmt14.en-de_gpu_titanxp.yml normal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA0eRJl8DZr2"
      },
      "source": [
        "If you want to get the performance of efficient architecture by extracting the weights from supernet (instead of using the standalone training done in Step 5), change the input from `baseline/effnet/checkpoint_best.pt` to `baseline/supernet/checkpoint_best.pt`.\n",
        "\n",
        "That's all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TVbj2bLDLu1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0opSWSHDUYrn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}